<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>向量的掩码训练 -- 交叉熵&amp;软标签</title>
    <link href="/2025/03/11/cel_smooth_labels/"/>
    <url>/2025/03/11/cel_smooth_labels/</url>
    
    <content type="html"><![CDATA[<h2 id="intro">intro</h2><blockquote><p>本次写得很烂、dky可能是最近身体欠佳吧，，肉体状态很影响精神状态进而拖累产出的一位女孩。</p></blockquote><h2 id="tldr">TL;DR</h2><p>如果你不知道为什么非得在掩码语言模型训练里面用inputs_embeds的话，我祝福你成功因为我没成功。当你改好了datacollator使得未被掩码的向量都有统一的target向量，在算交叉熵的时候记得把reduction设置成none，算出loss之后根据你的target向量将未被掩码的向量对应的loss值去掉之后再求平均进而再反向传播。如果只关心这个操作的话直接拉到最后看吧因为前面全是废话、</p><h2 id="什么问题----解决问题">什么问题 -- 解决问题</h2><p>其实并没有一开始就知道自己的问题是什么所以当时只是走到哪里算哪里，但写文总不能如此没有条理吧！所以先小小总结一番，原来是长版tldr！</p><p>在做什么：在做掩码语言模型的训练，但每个位点的输入不是经过分词器处理得到的tokenid，而是自行编码的四维向量。对于每个位点，训练模型输出一个四维logit值，这个四维logit向量做softmax计算后和对应的原始四维向量计算交叉熵损失值（pytorch的CrossEntropyLoss函数整合了这两个操作，后续简称该函数为cel）。</p><blockquote><p>额外烧烤：但！需要说明的是这里的自行编码四维向量只有有限种可能。在掩码训练任务中，我希望模型可以根据上下文的向量来预测出被掩码位置的向量是什么，即可以把其看作一个分类问题。但似乎也不是传统意义上的软分类问题……？搜索了一下<ahref="https://blog.csdn.net/zdt2018210321/article/details/133018783">发现</a>，在我的科学问题背景下，这个自行编码的四维向量似乎处在软标签和多标签的中间。举例说明：每个位点有两个信息点，对于这些信息点，会有四种类别。当一个位点的两个信息点同属一个类别（假设这个类别对应向量的第一维），则编码该位点为[1,0,0,0]；但当一个位点的两个信息点分属两个类别（假设分属向量第一二维），则编码该位点为[0.5,0.5,0,0]。既不是传统多标签的独立多元标签，也不是传统软标签的用概率分布表示属于某个类别的置信度（因为[0.5,0.5,0,0]是表示两个信息点分属两个类别，而不是单属于这两个类别其中的某一个的概率是0.5）（好绕，这对吗）（果然这个策略很怪），而是一种经过归一化的多标签（？），考虑了多类别之间比例的问题？</p><p>说了那么多，其实意思就是纠结了一下为什么用交叉熵做损失函数。</p></blockquote><p>最开始遇到的问题是：算出来的交叉熵损失值是很小的负值，是很小的很小，seriously。</p><p>自己魔改了什么：改了一个适配四维向量的DatacollatorforMaskedLM，主要改动点：继承原本对tokenid的处理方法，把未经掩码的向量对应的label（说到datacollator就忍不住用label，，为了和代码一致嘛，其实和本文里的target一个意思）改成[-100,-100,-100,-100]。</p><p>搜索后发现问题：输入为token id情况里，分类target为hardtarget，可以使用cel里的ignore_index参数来设置一个特定的数字，使得分类target为这个数字的输入不参与损失值和梯度的计算，以此在损失值计算中屏蔽target被datacollator设为-100的未经掩码token，达到只计算被掩码token的损失值的目标。但cel的ignore_index参数不适用与分类target为softtarget的情况（part1/2），所以将target为[-100,-100,-100,-100]的未经掩码向量也纳入了损失值计算的部分，计算出来的损失值异常小，也进一步影响了梯度计算。</p><p>解决问题：默认的cel损失值是由target非ignore_index的样本计算平均值得到的。为了未经掩码向量排除在损失值和梯度计算外，设置cel参数使其输出每个样本的损失值，去掉未经掩码向量的部分，再手动求平均（part3）。</p><h2 id="你咋了">-100你咋了</h2><p>又在用预设的向量做预训练（后续发现此技效果不止一般，简直是拖累，呵）。最开始试着预训练了几个epoch，发现loss值竟然是很小的负值……？完全困惑，根据交叉熵的公式正常来说不应该有负值啊！后来想了一圈发现自己之前改DatacollatorforMaskedLM的时候直接继承了tokenid里的思路：把没有被掩码的token对应的labels换成-100。虽然因为数学很差没有敏锐地发现这里的相关性，但之前用tokenid策略的时候也没有细想过自己从哪里复制下来的代码里为什么datacollator里要把没有被掩码的token对应的labels换成-100（甚至女孩一开始不是调用已有的库而是把源码复制下来自己改过来适配其他模型的，之后要用到时居然对原理一无所知ygirrrrl y），所以只是出于好奇去搜索了一下，才发现：</p><blockquote><p><ahref="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss">torch.nn.CrossEntropyLoss(weight=None,size_average=None, ignore_index=-100, reduce=None, reduction='mean',label_smoothing=0.0)</a></p></blockquote><p>这里交叉熵函数里有一个ignore_index的参数，文档中对这个参数的描述如下：</p><blockquote><p><strong>ignore_index</strong> (<ahref="https://docs.python.org/3/library/functions.html#int"title="(in Python v3.13)"><em>int</em></a>, <em>optional</em> ) –Specifies a target value that is ignored and does not contribute to theinput gradient. When <code>size_average</code> is <code>True</code>, theloss is averaged over non-ignored targets. Note that<code>ignore_index</code> is only applicable when the target containsclass indices.</p><p>注：size_average参数已弃用，默认为True</p></blockquote><p>所以计算交叉熵时，这些label为-100的部分不会参与梯度计算，只计算label为其他值的loss值，最后求并输出这些loss值的平均值(reduction这个参数默认为mean)。也就是说label为ignore_index值(未被掩码的token)的部分是不会参与到后续反向传播计算里面的，正好契合了maskedlanguagemodeling里只关心被掩码的token的loss有没有下降的任务目标。需要注意的是最后一句里说到的：ignore_index这个参数只在目标(是文档里提到的target，但前文描述时用了labels来描述，因为我改datacollator的代码里面用的变量就这个名字啊、好的为了保持一致性后面还是都用target吧)包含类别索引时才适用（这种target在后续的搜索结果中被冠以一个更专业的名词：hardtarget）。一个例子：假设语言模型最后输出的掩码位置上的logit是vocab_size维度的一个向量，pt的交叉熵函数会计算这个logit和这个掩码位置上原本的tokenid之间的交叉熵值，来评估模型预测的和真实值之间的差距。例子里的tokenid就是hard target，是对应token在vocab里的索引，只有一个数字。</p><p>而我的情况是，target是向量而非索引，这就意味着我的target不会是hardtarget了。因此，ignore_index这个参数就失效了，导致如下情况：我以为把未被掩码的向量对应的target修改成[-100,-100,-100,-100]之后pt的交叉熵函数就不会考虑这部分向量，但其实这部分向量在loss计算里大放异彩从而导致了loss值为超小负值（绝对值大概八位数吧呵呵什么时候8figure可以用来描述我的身家而非我的模型loss值呢、、、）服了。</p><h2 id="搜索中">搜索中</h2><p>其实当时也不知道该怎么搜索，可能就直接搜了cross entropyloss/pytorch/ignoreindex这几个词吧，搜到了第二个参考。其实现在回看发现这篇就已经全面解决我的问题了，但当时女孩的问题是根本不知道自己的问题是什么呵呵、所以就放下了继续搜搜。经过了再后面两篇帖子，女孩逛到了cel的issue11959页，然后津津有味地看起来了。（have 2admit我还没太理解github里issue的大概逻辑，就是怎么完整地梳理一个功能从一开始提出issue到那些开发人员在issue页面大讨论到最后功能落地这个过程呢？我总是找不到最源头那个issue在哪，但慢慢来吧）</p><p>让dsr1总结了一下issue11959大家都在干嘛，ds就在后台我们掌声把ta请出来吧：</p><blockquote><p>这个Issue是关于在PyTorch的<code>nn.CrossEntropyLoss</code>中增加对<strong>软目标分布（Soft TargetDistribution）</strong>的支持。以下是关键点总结：</p><hr /><h3 id="问题描述"><strong>问题描述</strong></h3><ul><li><strong>当前功能</strong> ：PyTorch的交叉熵损失(<code>nn.CrossEntropyLoss</code>) 仅支持 <strong>硬目标</strong>（即目标为类别索引，如 <code>[0, 2, 1]</code>）。</li><li><strong>用户需求</strong> ：希望在训练时支持<strong>软目标分布</strong> （即目标为概率分布，如<code>[[0.8, 0.1, 0.1], [0.2, 0.7, 0.1]]</code>）。软目标在防止过拟合、知识蒸馏等场景中非常有用。</li></ul><hr /><h3 id="数学原理"><strong>数学原理</strong></h3><ul><li><strong>硬目标</strong> ：损失为负对数似然，即<code>-y_c</code>，其中 <code>c</code> 是目标类别的索引。</li><li><strong>软目标</strong> ：实际是计算模型输出分布(<code>softmax(x)</code>) 和目标分布 <code>p</code> 的<strong>KL散度</strong> ，公式为：</li></ul><pre class="codeWrapper" data-code-lang=""><pre class="hljs"><code class="">Loss = -Σ p_i * log(softmax(x_i)) = Σ p_i * (log(p_i) - y_i) + constant  </code></pre></pre><p>其中 <code>y_i = log_softmax(x_i)</code>。优化时常数项可忽略。</p><hr /><h3 id="实现讨论"><strong>实现讨论</strong></h3><ul><li><strong>现有实现的限制</strong> ：<code>CrossEntropyLoss</code>假设目标是类别索引（形状为 <code>(N,)</code>的整数张量）。</li><li><strong>扩展方案</strong> ：</li></ul><ol type="1"><li><strong>修改现有损失函数</strong> ：允许目标张量为形状<code>(N, C)</code>的概率分布（浮点张量），通过检查目标形状或类型自动切换模式。</li><li><strong>新增单独损失函数</strong> ：例如<code>CrossEntropyLossWithSoftLabels</code>，专门处理软目标。</li></ol><ul><li><strong>争议点</strong> ：</li><li><strong>命名与计算对齐</strong>：交叉熵与KL散度的差异（是否丢弃常数项）。真实交叉熵不含常数项，而KL散度会包含，但这不影响梯度计算。</li><li><strong>兼容性</strong> ：原有参数如 <code>ignore_index</code>在软目标下的行为无意义，需在文档中说明。</li><li><strong>用户习惯</strong> ：Keras等框架通过不同函数区分（如<code>CategoricalCrossentropy</code> 和<code>SparseCategoricalCrossentropy</code>），PyTorch是否应保持一致性？</li></ul><hr /><h3 id="开发决策"><strong>开发决策</strong></h3><ul><li><strong>最终方案</strong> ：在原有 <code>CrossEntropyLoss</code>中扩展支持软目标，根据目标张量的形状（<code>(N,)</code> vs<code>(N, C)</code>）自动选择模式。</li><li><strong>关闭原因</strong> ：通过提交 <ahref="https://github.com/pytorch/pytorch/commit/a42345a">a42345a</a>解决了该问题，允许浮点型目标张量直接输入以实现软目标。</li></ul><hr /><h3 id="代码示例用户实现"><strong>代码示例（用户实现）</strong></h3><p>用户提供的临时实现：</p><pre class="codeWrapper" data-code-lang="python"><pre class="hljs"><code class="python">def softmax_cross_entropy_with_softtarget(input, target, reduction='mean'):    logprobs = torch.nn.functional.log_softmax(input, dim=1)    batchloss = - torch.sum(target * logprobs, dim=1)    if reduction == 'mean':        return torch.mean(batchloss)    elif reduction == 'sum':        return torch.sum(batchloss)    else:        return batchloss</code></pre></pre><hr /><h3 id="总结"><strong>总结</strong></h3><p>此功能已通过扩展 <code>nn.CrossEntropyLoss</code>实现对软目标的支持。用户可通过传入浮点型概率张量作为目标直接使用，无需额外实现。</p></blockquote><p>太好了ds，感谢你，请下台休息一下。其实这个issue和我的问题没啥关系，因为我就只是在知道自己用的是softtarget的情况下无念无想地用了cel，幸好这个issue早早地就提出来了不然被这个折磨的就是我了。</p><p>了解了背景之后就很好理解为什么ignore_index这个参数不能用在softtarget上了：ignore_index是为了离散类别标签设计的。如果一个样本的标签是2，设置ignore_index=2就会跳过这个样本的损失计算，这个操作是基于离散类别和ignore_index两个整数值的直接匹配（target== ignore_index）。softtarget作为概率分布，自然无法通过直接对比浮点数的概率张量和整数索引来决定是否忽略某些样本。</p><h2 id="所以女孩的问题要怎么解决">所以女孩的问题要怎么解决</h2><p>回看该帖发现其思路就是：输出每个样本的损失值（the unreducedloss），根据target值筛掉不要的“类别”，再手动求平均值。</p><p>贴上这个思路算损失值的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">loss_fn = CrossEntropyLoss(reduction=<span class="hljs-string">&#x27;none&#x27;</span>)<br>outputs = model(inputs)<br>logits = outputs[<span class="hljs-string">&#x27;logits&#x27;</span>]<br>loss = loss_fn(logits.view(-<span class="hljs-number">1</span>,logits.size(-<span class="hljs-number">1</span>)),labels.view(-<span class="hljs-number">1</span>,labels.size(-<span class="hljs-number">1</span>)))<br>loss = loss[labels.view(-<span class="hljs-number">1</span>,labels.size(-<span class="hljs-number">1</span>))[:, <span class="hljs-number">0</span>] != -<span class="hljs-number">100</span>].mean()<br></code></pre></td></tr></table></figure><blockquote><p>reduction ='none'参数：默认为mean，即对所有非ignore_index的样本求平均。none即不做reduction(聚合)处理，输出所有样本的损失。</p></blockquote><p>第一个loss是损失函数计算出来的形状为[批次大小*序列长度]的损失值，对应着每个向量的损失值。现在要将labels（即target）为[-100,-100,-100,-100]的未经掩码向量对应的损失值从这里去掉，再求平均值。因为前面计算loss的时候将logits和labels展平为二维张量，所以现在做布尔运算去掉相应损失值时也要把labels展平。<code>labels.view(-1,labels.size(-1))[:, 0] != -100</code>做了这样的操作：将形状为[批次大小，序列长度，维度]的labels展平为[批次大小*序列长度,维度]的二维张量，根据每个向量的labels的第一维是否不等于100返回一个布尔张量。这个布尔张量用于将loss里掩码的向量对应的loss值取出来（因为这些向量的labels第一维不等于-100，在布尔张量里对应True），最后取平均。</p><p>多么简单的一个操作！但女孩似乎罗里吧嗦说了一大堆、算了以后有心思再改吧。</p><h2 id="参考参考">参考参考</h2><p>函数文档：<ahref="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss">torch.nn.CrossEntropyLoss</a></p><p>最开始意欲参考但没搞懂的帖，最后发现就是答案：<ahref="https://discuss.pytorch.org/t/cross-entropy-loss-ignore-index/146784/">pytorchforum 146784 cross entropy loss ignore index</a></p><p>以为和我的问题一样准备欣喜若狂然后发现不是，但还是给了一些继续搜索的启发：<ahref="https://datascience.stackexchange.com/questions/45285/loss-function-for-probability-regression">stackexchange45285 loss function for probability regression</a></p><p>引导我去看相关issue的：<ahref="https://stackoverflow.com/questions/68907809/soft-cross-entropy-in-pytorch">stackoverflow68907809 soft cross entropy in pytorch</a></p><p>issues：</p><p><a href="https://github.com/pytorch/pytorch/issues/11959">11959(Sep22 2018，在这个issue下提到了最后解决的实现)</a></p><p><a href="https://github.com/pytorch/pytorch/issues/7455">7455(May 102018，一个更早的issue，讨论了很多)</a></p><p>plus: cross entropy loss mean weight</p><p><ahref="https://github.com/pytorch/pytorch/issues/31295">31295</a></p>]]></content>
    
    
    <categories>
      
      <category>note</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>roberta模型你在干嘛</title>
    <link href="/2025/02/08/robertawhy/"/>
    <url>/2025/02/08/robertawhy/</url>
    
    <content type="html"><![CDATA[<h2 id="intro">intro</h2><blockquote><p>roberta模型为何这样</p></blockquote><h2 id="tldr">TL;DR</h2><p>如果你在试图向huggingface的Roberta模型里输入inputs_embeds时遇到了IndexError的话：</p><ol type="1"><li>如果IndexError是和位置嵌入有关的那这篇可能有关；</li><li>模型源代码里由inputs_embeds生成position_ids会考虑padding_idx(pad_token_id)；</li><li>Roberta模型的原始配置RobertaConfig里pad_token_id为1，所以如果用户不输入自定义position_ids，生成的id是从2开始的。语料长度和最长长度相差不大的情况下，position_ids可能会和position_embedding冲突，从而导致IndexError；</li><li>不是那拉倒</li></ol><h2 id="问题">问题</h2><p>普遍的nlp构建模型流程（可能吧因为我不做自然语言）（DNA难道不是一种真正的自然语言……？）：得到待训练的文本-- 对文本做些预处理 -- 选择并训练适合的分词器 --用分词器对处理好的文本进行分词，得到词索引 --组织成可以批量输入到模型里的数据结构，开始训练。但我现在的科学问题已经进到了需要用奇技淫巧（有必要这么形容吗）来绕道的胡同，不知是福是祸啊！所以现在我想做的是：绕过抱抱脸里已经实现好的模型里已有embedding层，自己先给一个起始的嵌入。虽然不知道这对此科学问题到底增益几何，但技术问题已堂堂袭来！</p><h3 id="复现问题">复现问题</h3><p>对原始数据操作一番后，先取了一个batch的嵌入试试能不能跑通，其形状为<code>torch.Size([32, 512, 4])</code>。</p><p>根据roberta模型说明页面里面对inputs_embeds的说明：</p><blockquote><p>(<code>torch.FloatTensor</code> of shape<code>(batch_size, sequence_length, hidden_size)</code>,<em>optional</em> ) — Optionally, instead of passing<code>input_ids</code> you can choose to directly pass an embeddedrepresentation. This is useful if you want more control over how toconvert <code>input_ids</code> indices into associated vectors than themodel’s internal embedding lookup matrix.</p></blockquote><p>模型的参数设置：</p><blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">roberta_config = RobertaConfig(<br>vocab_size=<span class="hljs-number">20</span>,  <span class="hljs-comment">#没想好，先占个位</span><br>hidden_size=<span class="hljs-number">256</span>,  <br>num_hidden_layers=<span class="hljs-number">12</span>,  <br>num_attention_heads=<span class="hljs-number">8</span>,  <br>intermediate_size=<span class="hljs-number">1024</span>,  <br>max_position_embeddings=<span class="hljs-number">512</span>,    )<br></code></pre></td></tr></table></figure></blockquote><p>既然输入的张量最后一个维度要和模型的隐藏层维度相同，那么我就用了一个nn.linear把4维升到256维，得到<code>torch.Size([32, 512, 256])</code>的张量。</p><p>将这个张量输入到roberta模型后报错（截取部分报错信息）：</p><blockquote><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs routeros">---------------------------------------------------------------------------<br>IndexError                                Traceback (most recent call last)<br>Cell <span class="hljs-keyword">In</span>[21],line 1<br>----&gt; 1 out1 = roberta_lm_model(<span class="hljs-attribute">inputs_embeds</span>=out, <span class="hljs-attribute">output_hidden_states</span>=<span class="hljs-literal">True</span>,position_ids=position_ids)<br><br>File ~/path/<span class="hljs-keyword">to</span>/nn/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:1204, <span class="hljs-keyword">in</span> RobertaForMaskedLM.forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)labels (torch.LongTensor of shape (batch_size, sequence_length), optional):<br>-&gt;  outputs =self.roberta(<br>input_ids,<br><span class="hljs-attribute">attention_mask</span>=attention_mask,<br><span class="hljs-attribute">token_type_ids</span>=token_type_ids,<br><span class="hljs-attribute">position_ids</span>=position_ids,<br><span class="hljs-attribute">head_mask</span>=head_mask,<br><span class="hljs-attribute">inputs_embeds</span>=inputs_embeds,<br><span class="hljs-attribute">encoder_hidden_states</span>=encoder_hidden_states,<br><span class="hljs-attribute">encoder_attention_mask</span>=encoder_attention_mask,<br><span class="hljs-attribute">output_attentions</span>=output_attentions,<br><span class="hljs-attribute">output_hidden_states</span>=output_hidden_states,<br><span class="hljs-attribute">return_dict</span>=return_dict,<br>)<br>sequence_output = outputs[0]<br>prediction_scores =self.lm_head(sequence_output)<br><br>File ~/path/<span class="hljs-keyword">to</span>/nn/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:912, <span class="hljs-keyword">in</span> RobertaModel.forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)<br><span class="hljs-keyword">else</span>:<br>token_type_ids = torch.zeros(input_shape, <span class="hljs-attribute">dtype</span>=torch.long, <span class="hljs-attribute">device</span>=device)<br>--&gt; embedding_output =self.embeddings(<br><span class="hljs-attribute">input_ids</span>=input_ids,<br><span class="hljs-attribute">position_ids</span>=position_ids,<br><span class="hljs-attribute">token_type_ids</span>=token_type_ids,<br><span class="hljs-attribute">inputs_embeds</span>=inputs_embeds,<br><span class="hljs-attribute">past_key_values_length</span>=past_key_values_length,<br>)<br><span class="hljs-keyword">if</span> attention_mask isNone:<br>attention_mask = torch.ones((batch_size, seq_length + past_key_values_length), <span class="hljs-attribute">device</span>=device)<br><br>File ~/path/<span class="hljs-keyword">to</span>/nn/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:127, <span class="hljs-keyword">in</span> RobertaEmbeddings.forward(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)<br>embeddings = inputs_embeds + token_type_embeddings<br><span class="hljs-keyword">if</span> self.position_embedding_type ==<span class="hljs-string">&quot;absolute&quot;</span>:<br>--&gt; position_embeddings =self.position_embeddings(position_ids)<br>embeddings += position_embeddings<br>embeddings =self.LayerNorm(embeddings)<br><br>File ~/path/<span class="hljs-keyword">to</span>/nn/lib/python3.10/site-packages/torch/nn/functional.py:2551, <span class="hljs-keyword">in</span> embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)<br><span class="hljs-comment"># Note [embedding_renorm set_grad_enabled]</span><br><span class="hljs-comment"># <span class="hljs-doctag">XXX:</span> equivalent to</span><br><span class="hljs-comment"># with torch.no_grad():</span><br><span class="hljs-comment">#   torch.embedding_renorm_</span><br><span class="hljs-comment"># remove once script supports set_grad_enabled</span><br>no_grad_embedding_renorm(weight, input, max_norm, norm_type)<br>-&gt;  return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)IndexError: index out of range <span class="hljs-keyword">in</span> self<br></code></pre></td></tr></table></figure></blockquote><h3 id="报错了怎么办">报错了怎么办！</h3><p>女孩其实不懂怎么看报错信息、所以首先把所有报错（包括标准库的）全喂给ds并用最简单的问句【怎么解决】问了。乐的是ds虽然知道这是个索引超出范围问题，但一直在思维链里反复【不过在这里输入的序列长度是512，模型设置是512，应该没问题。】（刚解决此问题后得意忘形将对话记录删掉了啊啊啊啊让我试试能否重现），只能淡淡地又换了几个平台换了几种问问题方法（其实应该保留对话下来复盘一下应该怎么有效调教ai的但女孩全删掉了，女孩你这样是永远无法赚到卖课的钱的）。</p><p>问着问着虽然没能得到直接的解决方法，但从答案里还是可以知道：是position_ids这个变量里包含了超过模型允许的位置索引，从而导致了索引超出报错，可以着重看一下这句代码</p><blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">--&gt; <span class="hljs-number">127</span> position_embeddings =<span class="hljs-variable language_">self</span>.position_embeddings(position_ids)<br></code></pre></td></tr></table></figure></blockquote><p>我：之前用input_ids的时候也没遇到这个问题啊……但之前的序列长度由于分词器分词操作所以每句的长度不一而且都没有到512的，所以没出现这个问题。现在是每条序列的长度都为512，但模型的最大位置嵌入参数设置的也是512啊之前也没看到哪里说这个参数有什么设置的要求。出于懒、畏难情绪和对源代码的信任，女孩一开始并没有想着看看源代码在干嘛，而是尝试改了一下max_position_embeddings这个参数。结果为：改成513-- 继续报错，改成514 -- 不报错了。</p><p>？</p><p>到这里已经开始感觉：不会是索引起始和左闭右开吧……然后被叫去1on1了（女孩这是你的笔记还是日记？），谈话结束后摆烂一下午直至晚上才开始真正干活，终于回去看源代码这部分到底在干嘛，发现：</p><p>在<ahref="https://github.com/huggingface/transformers/blob/v4.48.2/src/transformers/models/roberta/modeling_roberta.py#L127">modeling_roberta.py</a>里找报错的第127行代码以及相关函数</p><blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-variable language_">self</span>.padding_idx = config.pad_token_id<br><span class="hljs-variable language_">self</span>.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size, padding_idx=<span class="hljs-variable language_">self</span>.padding_idx)<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self, input_ids=<span class="hljs-literal">None</span>, token_type_ids=<span class="hljs-literal">None</span>, position_ids=<span class="hljs-literal">None</span>, inputs_embeds=<span class="hljs-literal">None</span>, past_key_values_length=<span class="hljs-number">0</span></span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-keyword">if</span> position_ids <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">if</span> input_ids <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                <span class="hljs-comment"># Create the position ids from the input token ids. Any padded tokens remain padded.</span><br>                position_ids = create_position_ids_from_input_ids(input_ids, <span class="hljs-variable language_">self</span>.padding_idx, past_key_values_length)<br>            <span class="hljs-keyword">else</span>:<br>                position_ids = <span class="hljs-variable language_">self</span>.create_position_ids_from_inputs_embeds(inputs_embeds)<br><span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.position_embedding_type == <span class="hljs-string">&quot;absolute&quot;</span>:<br>--&gt; <span class="hljs-number">127</span>     position_embeddings = <span class="hljs-variable language_">self</span>.position_embeddings(position_ids)<br>            embeddings += position_embeddings<br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">create_position_ids_from_inputs_embeds</span>(<span class="hljs-params">self, inputs_embeds</span>):<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.</span><br><span class="hljs-string">Args:</span><br><span class="hljs-string">     inputs_embeds: torch.Tensor</span><br><span class="hljs-string"></span><br><span class="hljs-string">Returns: torch.Tensor</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        input_shape = inputs_embeds.size()[:-<span class="hljs-number">1</span>]<br>        sequence_length = input_shape[<span class="hljs-number">1</span>]<br><br>        position_ids = torch.arange(<br>            <span class="hljs-variable language_">self</span>.padding_idx + <span class="hljs-number">1</span>, sequence_length + <span class="hljs-variable language_">self</span>.padding_idx + <span class="hljs-number">1</span>, dtype=torch.long, device=inputs_embeds.device<br>        )<br>        <span class="hljs-keyword">return</span> position_ids.unsqueeze(<span class="hljs-number">0</span>).expand(input_shape)<br></code></pre></td></tr></table></figure></blockquote><p>既然第127代码导致了报错，那里面的position_ids是否也有问题？一开始我没有输入position_ids，所以RobertaEmbedding类里的函数create_position_ids_from_inputs_embeds生成了一个。后面ai又建议我自己创建一个position_ids(代码如下)，然后就没有报错了……怎么回事啊你们自己还和自己打架？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">position_ids = torch.arange(<span class="hljs-number">512</span>).expand(batch_size, -<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><h3 id="w-h-y">W H Y ?</h3><p>两种方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-variable language_">self</span>.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size, padding_idx=<span class="hljs-variable language_">self</span>.padding_idx) <span class="hljs-comment">#源代码</span><br>position_embeddings = <span class="hljs-variable language_">self</span>.position_embeddings(position_ids)<span class="hljs-comment"># ai建议</span><br></code></pre></td></tr></table></figure><table><thead><tr><th>源代码</th><th>ai建议</th></tr></thead><tbody><tr><td>我没有输入pad_token_id（因为确实不需要pad）<br />所以对于nn.Embedding来说padding_idx的默认值是0<br />由此这个函数生成的position_ids从1开始，[1,2,3,……,512]</td><td>从0开始，[0,1,2,……,511]</td></tr></tbody></table><p>aaaa果然是0based和1based你们两个eeeeeee</p><p>那为什么0开始的id可以在此成功运行而1开始的不行</p><h4 id="nn.embedding你又在干嘛">nn.Embedding你又在干嘛</h4><p>nn.embedding是pytorch的一个嵌入层，主要用于<strong>将离散的索引（如单词ID、类别ID）映射到连续的高维向量空间</strong>。其输入通常是类别型数据(categoricaldata)，比如nlp里的tokenid，推荐系统里的用户id等等。通俗来说就是一个可训练的查找表(lookuptable)，你给它一个整数索引，它返回这个索引对应的嵌入向量。</p><p>那么self.position_embeddings也就生成了一个表大小有512(max_position_embeddings)，维度有256(hidden_size)的表。这里的表大小不是word_embedding里面词汇表的大小，而是一条序列长度的大小（把词汇表里每个token替换成一条序列里每个位置）。下面调用这个表的时候，输入的索引就是position_ids。</p><p>embedding的表索引从0开始，所以源代码的id最后一个索引512在表里就没有对应的嵌入了，所以就indexerror了。</p><h4 id="又是啥意思">513又是啥意思</h4><p>但如果如上所述那为什么要改到514才正常……？难道不是513就应该正常了吗</p><p>女孩终于点开了解决问题最应该打开的issue页面，并搜索roberta positionids。然后发现roberta模型由于从fairseq那继承过来的时候没有改动原有的模型配置，其padding_idx的默认值其实是：1！恍然大悟的女孩终于发现了其实应该看看RobertaConfig的默认值……</p><p>ps：上面源代码里写的默认值其实是我想知道如果我不输入pad_token_id的时候默认值是什么，女孩直接去搜nn.embedding的默认值了，压根没有想到还有一个RobertaConfig……那么不输入pad_token_id时，源代码得到的position_ids其实是从2开始的。。</p><h3 id="所以怎么解决类似问题">所以怎么解决类似问题</h3><ol type="1"><li>自己输入position_ids</li><li>在config里把pad_token_id设置为-1(仅当你不用padtoken时。如果要用：自己构建position_ids/根据语料长度调整最长长度)</li></ol><h3 id="复盘">复盘</h3><p>小小问题竟浪费女孩这么多时间那么为了之后浪费的时间可以少一点我们认为需要复盘一下，但现在不想写，之后有空再更新吧(女孩你别一鸽就是永远)</p><h4 id="ps-再补充一下position-ids">ps 再补充一下position ids</h4><p>问了一下ds为什么根据inputs_embeds创建position_ids时（即函数<code>create_position_ids_from_inputs_embeds</code>）要考虑padding_idx。ds大思考一分多钟后答案总结如下：</p><blockquote><p>在Roberta模型中，创建位置IDs时考虑<code>padding_idx</code>的主要原因是为了避免生成的位置ID与嵌入层中的填充索引冲突，确保位置嵌入的有效性。具体原因如下：</p><ol type="1"><li><strong>避免位置嵌入的无效化</strong> ：位置嵌入层（<code>Embedding</code>）中的<code>padding_idx</code>对应的向量通常被设置为零且不参与训练。若位置ID等于<code>padding_idx</code>，该位置的位置嵌入会被固定为零，失去学习能力。通过从<code>padding_idx + 1</code>开始生成位置ID（如<code>padding_idx=0</code>时，ID从1开始），所有有效位置均获得非填充索引，确保位置嵌入被正常学习和使用。</li><li><strong>处理输入的不确定性</strong> ： 当直接提供<code>inputs_embeds</code>时，模型无法区分哪些位置是填充的（因嵌入已生成，原始输入信息丢失）。此时，函数<strong>默认所有位置有效</strong> ，生成连续的位置ID，并通过避开<code>padding_idx</code>确保即使存在填充，位置嵌入也不会错误引用填充索引。</li><li><strong>保持一致性</strong> ： 在标准流程中，模型会根据<code>input_ids</code>处理填充位置（如将填充token的位置ID设为0）。但当无法获取<code>input_ids</code>时，此函数采用保守策略，生成不与<code>padding_idx</code>冲突的ID，以模拟非填充场景，避免引入意外偏差。</li></ol></blockquote><p>所以确实和input_ids时的处理稍有不同，下面是id对应的函数<code>create_position_ids_from_input_ids</code>请看：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_position_ids_from_input_ids</span>(<span class="hljs-params">input_ids, padding_idx, past_key_values_length=<span class="hljs-number">0</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols</span><br><span class="hljs-string">    are ignored. This is modified from fairseq&#x27;s `utils.make_positions`.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        x: torch.Tensor x:</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns: torch.Tensor</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.</span><br>    mask = input_ids.ne(padding_idx).<span class="hljs-built_in">int</span>()<br>    incremental_indices = (torch.cumsum(mask, dim=<span class="hljs-number">1</span>).type_as(mask) + past_key_values_length) * mask<br>    <span class="hljs-keyword">return</span> incremental_indices.long() + padding_idx<br></code></pre></td></tr></table></figure><p>那么当padding_idx设置为3的时候，位置id就从4开始。那前面的0，1，2怎么办呢？ds说为了避免和填充索引冲突，这三个就不会被用作位置id了。</p><p>*不知道其他模型的实现里是否有这些问题，但请注意</p><h2 id="参考参考">参考参考</h2><p><ahref="https://github.com/huggingface/transformers/blob/v4.48.2/src/transformers/models/roberta/modeling_roberta.py#L1669">roberta源代码</a></p><p><ahref="https://github.com/huggingface/transformers/issues/10736#issuecomment-800175342">robertapad tokens</a></p>]]></content>
    
    
    <categories>
      
      <category>note</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>vcf2fasta</title>
    <link href="/2025/01/15/vcf2fasta/"/>
    <url>/2025/01/15/vcf2fasta/</url>
    
    <content type="html"><![CDATA[<h2 id="intro">intro</h2><blockquote><p>太好了！女孩只有在ddl迫近的时候才会想起来写这些、</p><p>其实有很多篇但不知为何有种一次就要搞篇大的但其实真的发了的只有一篇捡来的</p><p>所以又在捡！</p></blockquote><h2 id="tldr">TL;DR</h2><p>根据需要的fasta内容选软件，本文关心：</p><ol type="1"><li>上下游n bp</li><li>多态性位点所在元件整条序列</li></ol><p>不行的话找师兄师姐老师要序列吧、</p><h2 id="这是在干什么">这是在干什么</h2><p><strong>我们现在有一个存储了群体变异信息的vcf文件，但由于各种原因和下游应用，需要取出变异位点两侧的序列。</strong></p><p>作为存储变异信息的文件格式，vcf文件保存的信息可以说是位点信息。从位点信息到序列信息，需要做到的就是在检测变异时所用到的参考基因组里定位到相应位点，再按照后续对这个序列的操作目的来取包含这个位点的上下游序列(意思是你是想取这个多态性位点所在的基因/CDS或者什么特定的片段，还是你只是关心这个位点的邻居们都是些什么货色)。很简单的思路！不太确定自己写的工作量有多少，但既然别人都写过了为什么不拿、什么都写只会让你成为一个很厉害的写代码的我希望抱大腿的人！</p><h2 id="怎么做">怎么做</h2><h3 id="jvarkit----java-utilities-for-bioinformatics">jvarkit -- Javautilities for Bioinformatics</h3><p>在BioStar里绝望查找时找到的和当时我的需求最匹配的一个包，呵呵虽然之后还是又改了方向不再使用但还是献上我的感激之情！此包基于java开发，有超多功能简单总结这个包怎么用：</p><ol type="1"><li><p>先把参考基因组文件拿到！</p></li><li><p>jvarkit里的biostar251649jar包会在原vcf文件的基础上，在vcf的数据部分里的INFO部分加上两个字段：SEQ3_30和SEQ5_30(30是上下游序列的长度，可改)，输出相应的新vcf文件。</p></li><li><p>jvarkit里的bioalcidaejdk*jar包可以把这个新vcf文件里的两个新字段提取出来，和变异信息组成序列</p><p>*你以为这是脸滚键盘其实此包是java-based version of awk ofbioinformatics，bio和jdk好理解，Alcidae(海雀科)？因为海雀的英文是auk…just sooo random! 引用来自文档(女孩你还专门去看了…？)</p></li></ol><blockquote><p><strong>Why this name</strong></p><p>As 'bioalcidae' looks like an 'awk' for bioinformatics, we used '<ahref="https://en.wikipedia.org/wiki/Alcidae">Alcidae</a>', the taxonomicFamily of the '<a href="https://en.wikipedia.org/wiki/Auk">auk</a>'species.</p></blockquote><p>根据实际调试发现：此包对变异是SNP还是Indel，是双等位还是多等位都不敏感（下面例子可得）。</p><p>例子：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">## 得到的新vcf</span><br>$ java -jar dist/biostar251649.jar -n 10 -R tests/ref.fa tests/mutations.vcf<br><span class="hljs-comment">##INFO=&lt;ID=SEQ3_10,Number=1,Type=String,Description=&quot;Sequence on the 3&#x27; of mutation&quot;&gt;</span><br><span class="hljs-comment">##INFO=&lt;ID=SEQ5_10,Number=1,Type=String,Description=&quot;Sequence on the 5&#x27; of mutation&quot;&gt;</span><br>(...)<br><span class="hljs-comment">#CHROM  POS ID  REF ALT QUAL    FILTER  INFO    FORMAT  S1  S2  S3  S4</span><br>rotavirus   51  .   A   G   22.55   .   AC1=2;AF1=0.25;BQB=1;DP=944;DP4=849,0,93,0;FQ=23.7972;G3=0.75,0,0.25;HWE=0.033921;MQ=60;MQ0F=0;MQB=1;PV4=1,1,1,1;RPB=0.993129;SEQ3_10=GATGGTAAGC;SEQ5_10=TCTACTCAGC;SGB=-61.9012;VDB=3.53678e-05    GT:PL   0/0:0,255,134   0/0:0,255,127   0/0:0,255,137   1/1:70,255,0<br>rotavirus   91  .   A   T   5.45    .   AC1=1;AF1=0.124963;BQB=0.951201;DP=1359;DP4=1134,0,225,0;FQ=5.8713;MQ=60;MQ0F=0;MQB=1;PV4=1,4.80825e-05,1,1;RPB=0.0393173;SEQ3_10=GTTGTTGCTG;SEQ5_10=TTGAAGCTGC;SGB=-369.163;VDB=0.313337   GT:PL   0/0:0,255,133   0/1:40,0,31 0/0:0,255,134   0/0:0,255,82<br><br><span class="hljs-comment">## 取序列</span><br>java -jar dist/bioalcidaejdk.jar -F VCF -e <span class="hljs-string">&#x27;stream().forEach(V-&gt;println(&quot;&gt;&quot;+V.getContig()+&quot;:&quot;+V.getStart()+&quot;\n&quot;+V.getAttribute(&quot;SEQ5_20&quot;)+&quot;[&quot;+V.getAlleles().stream().map(A-&gt;A.getDisplayString()).collect(Collectors.joining(&quot;/&quot;))+&quot;]&quot;+V.getAttribute(&quot;SEQ3_20&quot;)));&#x27;</span><br><br>&gt;rotavirus:51<br>TGGTCGATTGCTCTATTGAA[A/G]AATTTCCATTGATGGCTAAA <br><span class="hljs-comment"># 这里会把包括参考基因型和变异基因型都列在[]里，所以说对snp/indel，双/多等位不会很敏感</span><br></code></pre></td></tr></table></figure><p>详解第二步代码的-e(来自作者在biostars 334253里的回答)</p><blockquote><p><code>stream()</code> convert to a stream of variant</p><p><code>forEach(V-&gt;println("&gt;"</code> for each variant startwriting the fasta header</p><p><code>V.getContig()+":"+V.getStart()+"\n"</code> write the fastaheader</p><p><code>V.getAttribute("SEQ5_20")</code> write 5' seq</p><p><code>["+V.getAlleles().stream().map(A-&gt;A.getDisplayString()).collect(Collectors.joining("/"))+"]"</code>write all alleles separated with '/'</p><p><code>V.getAttribute("SEQ3_20")))</code> write 3' seq</p></blockquote><p>我自己批量处理vcf的一段代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><code class="hljs bash">/#!/bin/bash<br><span class="hljs-comment">#SBATCH --job-name=          # 作业名称</span><br><span class="hljs-comment">#SBATCH --output=        # 标准输出文件</span><br><span class="hljs-comment">#SBATCH --error=         # 标准错误文件</span><br><span class="hljs-comment">#SBATCH --time=                     # 运行时间（格式：HH:MM:SS）</span><br><span class="hljs-comment">#SBATCH --ntasks=1                          # 分配任务数</span><br><span class="hljs-comment">#SBATCH --cpus-per-task=4                   # 每个任务分配的CPU数量（根据需求调整）</span><br><span class="hljs-comment">#SBATCH --mem=16G                           # 分配内存（根据需求调整）</span><br><br><br><span class="hljs-comment"># 设置脚本在遇到错误时停止</span><br><span class="hljs-built_in">set</span> -e<br><br><span class="hljs-comment"># 样品文件路径</span><br>SAMPLE_FILE=<span class="hljs-string">&quot;/path/to/sample/file&quot;</span><br><br><span class="hljs-comment"># VCF 输入文件路径</span><br>VCF_INPUT=<span class="hljs-string">&quot;/path/to/vcf&quot;</span><br><br><span class="hljs-comment"># 基因组参考序列路径</span><br>REFERENCE_GENOME=<span class="hljs-string">&quot;/path/to/fna&quot;</span><br><br><span class="hljs-comment"># JVarkit 工具路径</span><br>JVARKIT_JAR=<span class="hljs-string">&quot;/path/to/jvarkit.jar&quot;</span><br><br><span class="hljs-comment"># 输出目录（可选，建议统一输出到一个目录）</span><br>OUTPUT_DIR=<span class="hljs-string">&quot;/path/to/output/dir&quot;</span><br><span class="hljs-built_in">mkdir</span> -p <span class="hljs-string">&quot;<span class="hljs-variable">$OUTPUT_DIR</span>&quot;</span><br><br><span class="hljs-comment"># 读取每个样品并处理</span><br><span class="hljs-keyword">while</span> IFS= <span class="hljs-built_in">read</span> -r SAMPLE || [[ -n <span class="hljs-string">&quot;<span class="hljs-variable">$SAMPLE</span>&quot;</span> ]]; <span class="hljs-keyword">do</span><br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;开始处理样品: <span class="hljs-variable">$SAMPLE</span>&quot;</span><br><br>    <span class="hljs-comment"># 步骤 1: 使用 bcftools 提取样品 VCF</span><br>    bcftools view -Ov -s <span class="hljs-string">&quot;<span class="hljs-variable">$SAMPLE</span>&quot;</span> <span class="hljs-string">&quot;<span class="hljs-variable">$VCF_INPUT</span>&quot;</span> -o <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;OUTPUT_DIR&#125;</span>/<span class="hljs-variable">$&#123;SAMPLE&#125;</span>.vcf&quot;</span><br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;生成 <span class="hljs-variable">$&#123;SAMPLE&#125;</span>.vcf&quot;</span><br><br>    <span class="hljs-comment"># 步骤 2: 使用 GATK 选择变异 (主要是想取每个样品的变异)</span><br>    gatk SelectVariants -V <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;OUTPUT_DIR&#125;</span>/<span class="hljs-variable">$&#123;SAMPLE&#125;</span>.vcf&quot;</span> --exclude-non-variants -O <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;OUTPUT_DIR&#125;</span>/<span class="hljs-variable">$&#123;SAMPLE&#125;</span>_variant.vcf&quot;</span><br>    <span class="hljs-comment">#echo &quot;生成 $&#123;SAMPLE&#125;_variant.vcf&quot;</span><br><br>    <span class="hljs-comment"># 步骤 3: 使用 JVarkit 提取 flanking regions</span><br>    java -jar <span class="hljs-string">&quot;<span class="hljs-variable">$JVARKIT_JAR</span>&quot;</span> biostar251649 \<br>        -R <span class="hljs-string">&quot;<span class="hljs-variable">$REFERENCE_GENOME</span>&quot;</span> \<br>        <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;OUTPUT_DIR&#125;</span>/<span class="hljs-variable">$&#123;SAMPLE&#125;</span>.vcf&quot;</span> -n 50 &gt; <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;OUTPUT_DIR&#125;</span>/<span class="hljs-variable">$&#123;SAMPLE&#125;</span>_flank.vcf&quot;</span><br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;生成 <span class="hljs-variable">$&#123;SAMPLE&#125;</span>_flank.vcf&quot;</span><br><br>    <span class="hljs-comment"># 步骤 4: 使用 JVarkit 生成 FASTA 文件</span><br>    <span class="hljs-comment"># 如果改了上下游长度，这里也记得改</span><br>    java -jar <span class="hljs-string">&quot;<span class="hljs-variable">$JVARKIT_JAR</span>&quot;</span> bioalcidaejdk \<br>        -F VCF -e <span class="hljs-string">&#x27;stream().forEach(V-&gt;println(&quot;&gt;&quot;+V.getContig()+&quot;:&quot;+V.getStart()+&quot;\n&quot;+V.getAttribute(&quot;SEQ5_50&quot;)+&quot;[&quot;+V.getAlleles().stream().map(A-&gt;A.getDisplayString()).collect(Collectors.joining(&quot;/&quot;))+&quot;]&quot;+V.getAttribute(&quot;SEQ3_50&quot;)));&#x27;</span> \<br>        <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;OUTPUT_DIR&#125;</span>/<span class="hljs-variable">$&#123;SAMPLE&#125;</span>_flank.vcf&quot;</span> &gt; <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;OUTPUT_DIR&#125;</span>/<span class="hljs-variable">$&#123;SAMPLE&#125;</span>_flank.fasta&quot;</span><br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;生成 <span class="hljs-variable">$&#123;SAMPLE&#125;</span>_flank.fasta&quot;</span><br><br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;完成处理样品: <span class="hljs-variable">$SAMPLE</span>&quot;</span><br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;-----------------------------------------&quot;</span><br><br><span class="hljs-keyword">done</span> &lt; <span class="hljs-string">&quot;<span class="hljs-variable">$SAMPLE_FILE</span>&quot;</span><br><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;所有样品处理完成！&quot;</span><br></code></pre></td></tr></table></figure><h3 id="vcf2fasta">vcf2fasta</h3><p>我猜这个应该是之前用得比较多的一个……？但不知道之前大家用来干什么。和上面的包的主要区别是这个包会根据gff文件里面的基因组特征提取序列，所以这个包的序列提取是以基因组特征为主的，和上面以多态性位点为主不同。</p><p>翻找自己的文件发现早就把使用此包的代码删掉了，因为当时用了之后结果……【前情提要：当时的需求是取所有样品某个基因的序列，在有测序数据的前提下其实可以用组装数据来做但我没有！所以就先试着根据有的群体vcf文件和参考序列来取了】不能说是错吧，可能和用的gff文件里面注释有关系，取出来的蛋白质编码基因不是起始密码子开头，影响了后续的计算！回首过往觉得如果要解决的话应该是回头确认gff文件对目标基因的注释到底是从哪开始，对应的具体序列又是什么。虽然软件对CDS的inframe是有参数设定的，但我需要整个基因！当时因为太赶了最后通过最简单也是最难的【问师兄要组装好的序列】解决了呵呵。所以后续如果还需要使用请注意。</p><p>怎么用：</p><ol type="1"><li><p>准备参考序列，需要参考基因组的fa文件和gff文件。参考基因组的fa文件需要用samtools进行index；如果你的gff文件非常有个人特色的话，去github看看符不符合软件的要求。vcf文件也要index，但用tabix。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">samtools faidx ref.fa<br>bgzip my_vcf_file.vcf<br>tabix my_vcf_file.vcf.gz<br></code></pre></td></tr></table></figure></li><li><p>下载本体、pysam和art包</p></li><li><p>最简单的用法，其他参数按-h或者看github吧：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">python vcf2fasta.py -f genome.fas -v variants.vcf.gz -g intervals.gff -e CDS<br></code></pre></td></tr></table></figure></li><li><p>设定了-e CDS之后会输出所有的CDS，，希望之后能有用</p></li></ol><h2 id="上面的链接">上面的链接🔗</h2><p><a href="https://github.com/lindenb/jvarkit">jvarkit_github</a></p><p><ahref="https://jvarkit.readthedocs.io/en/latest/">jvarkit_document</a></p><p><ahref="https://github.com/santiagosnchez/vcf2fasta">vcf2fasta</a></p><p>BioStars的帖子</p><p><a href="https://www.biostars.org/p/251649/">251649</a></p><p><a href="https://www.biostars.org/p/334253/">334253</a></p><h2 id="结束">结束、</h2><p>此地的一个好处是写正经笔记的同时女孩还是不由自主散发出一种absent-minded的语言风格，但如果你的名字不是没有人的话你不会在意。</p>]]></content>
    
    
    <categories>
      
      <category>note</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>为什么python输出在out文件里不显示啊！！</title>
    <link href="/2024/12/10/flush_slurm/"/>
    <url>/2024/12/10/flush_slurm/</url>
    
    <content type="html"><![CDATA[<h2 id="此竟然为第一篇">此竟然为第一篇、</h2><blockquote><p>其实断断续续有五篇草稿但最后还是先把捡来的东西发了、、、</p></blockquote><p>为什么投完任务之后我的进度输出不显示在out文件里啊！！——来自：要处理多个样品本来以为一晚上肯定结束早上上工输入squeue后发现仍遥遥无期终于在18个小时后认为等待是高风险【主要还是老师降临视察后感到大祸临头】于是加了两条输出意图监控进度但发现1小时过去out文件干净得如同脑子的一位</p><p>被光盘头糊弄两个来回后发现咕狗给我的答案才是正确的，实践后痛斥光盘头得到以下总结，之后请留意：</p><h2 id="tldr">TL;DR</h2><p>用python -u urscript.py</p><h2 id="是缓冲区">是缓冲区！</h2><p>Python 的 <code>print()</code>函数将内容输出到标准输出（通常是终端或文件），但是它并不是立即将内容写入屏幕或文件中。为了提高效率，Python会将输出内容缓存起来，然后在适当的时候（如缓冲区满时）才将这些内容一次性地输出。</p><blockquote><p>输出缓冲区的三种方式：</p><ol type="1"><li><strong>行缓冲</strong> ：当输出中包含换行符（例如<code>print("Hello")</code>）时，Python会自动刷新缓冲区并输出内容。通常在交互式环境（如终端）中，<code>print()</code>会及时显示。</li><li><strong>全缓冲</strong> ：对于文件和重定向到文件的标准输出，Python默认会使用全缓冲模式，即当缓冲区满时才会写入文件。这意味着如果你在文件中输出，直到缓冲区满或者程序结束，才会将内容写入文件。</li><li><strong>无缓冲</strong> ：无缓冲模式下，<code>print()</code>函数会立即将输出内容写入文件或显示到终端。你可以通过设置 Python的输出流为无缓冲来避免缓冲的问题。</li></ol></blockquote><h2 id="缓冲区为什么导致此问题">缓冲区为什么导致此问题</h2><p>在提交作业时（比如使用 <code>sbatch</code>），Slurm会将标准输出和标准错误输出重定向到文件中。如果 Python使用了缓冲输出，而脚本没有结束或者没有写入换行符，缓冲区的内容可能不会被及时写入输出文件。这样，你就无法在日志文件中看到<code>print</code> 输出。</p><h2id="解决办法虽然都是一回事但仍然摆出三条like-why因为是笔记">解决办法（虽然都是一回事但仍然摆出三条，likewhy（因为是笔记</h2><ol type="1"><li>显式刷新缓冲区： 可以通过在 <code>print()</code> 语句后面添加flush=True 来强制 Python 在每次输出时刷新缓冲区。例如：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;sample_name&#125;</span> processed&quot;</span>, flush=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>这样确保每次<code>print()</code>输出都会刷新缓冲区，从而输出到out文件里</p><ol start="2" type="1"><li>用 sys.stdout.flush()： 也可以显式地使用<code>sys.stdout.flush()</code>来刷新缓冲区。这可以在脚本的任何地方调用，以确保输出被及时刷新到标准输出。例如：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> sys<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;sample_name&#125;</span> processed&quot;</span>)<br>sys.stdout.flush()  <span class="hljs-comment"># 强制刷新缓冲区</span><br></code></pre></td></tr></table></figure><ol start="3" type="1"><li>禁用缓冲：如果要在所有输出中都禁用缓冲，则可以直接在运行脚本时使用-u参数，让python以无缓冲模式运行</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">python -u my_script.py<br></code></pre></td></tr></table></figure><ol start="4" type="1"><li>使用日志模块：<em>(怎么回事不是只有三条吗！)(因为和上面三个不一样)</em>可以考虑用python的<code>logging</code>模块<em>(没用过)</em>，此模块允许你记录调试、信息、警告、错误和致命错误<em>(wow)</em>等不同级别日志，可以配置其使得输出立即写入文件/定期刷新</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> logging<br><br>logging.basicConfig(level=logging.DEBUG, <span class="hljs-built_in">format</span>=<span class="hljs-string">&#x27;%(asctime)s - %(message)s&#x27;</span>, handlers=[logging.StreamHandler(), logging.FileHandler(<span class="hljs-string">&#x27;job_output.log&#x27;</span>)])<br>logging.debug(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;sample_name&#125;</span> processed&quot;</span>)<br><span class="hljs-comment"># 谁看懂了、</span><br></code></pre></td></tr></table></figure><h2 id="参考链接">参考链接🔗</h2><p><ahref="https://blog.csdn.net/MissShihong/article/details/107862293">同病相怜文</a></p><p>&amp;光盘头老师</p><h2 id="结束-退朝">结束 退朝</h2>]]></content>
    
    
    <categories>
      
      <category>note</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>hello world</title>
    <link href="/2024/10/05/hello-world/"/>
    <url>/2024/10/05/hello-world/</url>
    
    <content type="html"><![CDATA[<h3 id="mood">mood:</h3><p><em>陷入一种小时候拿到一个漂亮本子却一时不知道要在上面写什么的期待和踌躇感。</em></p><blockquote><p>sorry but 本人的所有开头大都如此潦草</p></blockquote><p>人为给自己创造了一个新需求，随便放点。</p><p>主要是一些笔记【hopeso】，这位小姐最好不要把这里也搞成自己的ego和坏情绪随意发射地、、</p>]]></content>
    
    
    <categories>
      
      <category>random</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
