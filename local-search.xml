<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>roberta模型你在干嘛</title>
    <link href="/2025/02/08/robertawhy/"/>
    <url>/2025/02/08/robertawhy/</url>
    
    <content type="html"><![CDATA[<h2 id="intro">intro</h2><blockquote><p>roberta模型为何这样</p></blockquote><h2 id="tldr">TL;DR</h2><p>如果你在试图向huggingface的Roberta模型里输入inputs_embeds时遇到了IndexError的话：</p><ol type="1"><li>如果IndexError是和位置嵌入有关的那这篇可能有关；</li><li>模型源代码里由inputs_embeds生成position_ids会考虑padding_idx(pad_token_id)；</li><li>Roberta模型的原始配置RobertaConfig里pad_token_id为1，所以如果用户不输入自定义position_ids，生成的id是从2开始的。语料长度和最长长度相差不大的情况下，position_ids可能会和position_embedding冲突，从而导致IndexError；</li><li>不是那拉倒</li></ol><h2 id="问题">问题</h2><p>普遍的nlp构建模型流程（可能吧因为我不做自然语言）（DNA难道不是一种真正的自然语言……？）：得到待训练的文本-- 对文本做些预处理 -- 选择并训练适合的分词器 --用分词器对处理好的文本进行分词，得到词索引 --组织成可以批量输入到模型里的数据结构，开始训练。但我现在的科学问题已经进到了需要用奇技淫巧（有必要这么形容吗）来绕道的胡同，不知是福是祸啊！所以现在我想做的是：绕过抱抱脸里已经实现好的模型里已有embedding层，自己先给一个起始的嵌入。虽然不知道这对此科学问题到底增益几何，但技术问题已堂堂袭来！</p><h3 id="复现问题">复现问题</h3><p>对原始数据操作一番后，先取了一个batch的嵌入试试能不能跑通，其形状为<code>torch.Size([32, 512, 4])</code>。</p><p>根据roberta模型说明页面里面对inputs_embeds的说明：</p><blockquote><p>(<code>torch.FloatTensor</code> of shape<code>(batch_size, sequence_length, hidden_size)</code>,<em>optional</em> ) — Optionally, instead of passing<code>input_ids</code> you can choose to directly pass an embeddedrepresentation. This is useful if you want more control over how toconvert <code>input_ids</code> indices into associated vectors than themodel’s internal embedding lookup matrix.</p></blockquote><p>模型的参数设置：</p><blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">roberta_config = RobertaConfig(<br>vocab_size=<span class="hljs-number">20</span>,  <span class="hljs-comment">#没想好，先占个位</span><br>hidden_size=<span class="hljs-number">256</span>,  <br>num_hidden_layers=<span class="hljs-number">12</span>,  <br>num_attention_heads=<span class="hljs-number">8</span>,  <br>intermediate_size=<span class="hljs-number">1024</span>,  <br>max_position_embeddings=<span class="hljs-number">512</span>,    )<br></code></pre></td></tr></table></figure></blockquote><p>既然输入的张量最后一个维度要和模型的隐藏层维度相同，那么我就用了一个nn.linear把4维升到256维，得到<code>torch.Size([32, 512, 256])</code>的张量。</p><p>将这个张量输入到roberta模型后报错（截取部分报错信息）：</p><blockquote><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs routeros">---------------------------------------------------------------------------<br>IndexError                                Traceback (most recent call last)<br>Cell <span class="hljs-keyword">In</span>[21],line 1<br>----&gt; 1 out1 = roberta_lm_model(<span class="hljs-attribute">inputs_embeds</span>=out, <span class="hljs-attribute">output_hidden_states</span>=<span class="hljs-literal">True</span>,position_ids=position_ids)<br><br>File ~/path/<span class="hljs-keyword">to</span>/nn/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:1204, <span class="hljs-keyword">in</span> RobertaForMaskedLM.forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)labels (torch.LongTensor of shape (batch_size, sequence_length), optional):<br>-&gt;  outputs =self.roberta(<br>input_ids,<br><span class="hljs-attribute">attention_mask</span>=attention_mask,<br><span class="hljs-attribute">token_type_ids</span>=token_type_ids,<br><span class="hljs-attribute">position_ids</span>=position_ids,<br><span class="hljs-attribute">head_mask</span>=head_mask,<br><span class="hljs-attribute">inputs_embeds</span>=inputs_embeds,<br><span class="hljs-attribute">encoder_hidden_states</span>=encoder_hidden_states,<br><span class="hljs-attribute">encoder_attention_mask</span>=encoder_attention_mask,<br><span class="hljs-attribute">output_attentions</span>=output_attentions,<br><span class="hljs-attribute">output_hidden_states</span>=output_hidden_states,<br><span class="hljs-attribute">return_dict</span>=return_dict,<br>)<br>sequence_output = outputs[0]<br>prediction_scores =self.lm_head(sequence_output)<br><br>File ~/path/<span class="hljs-keyword">to</span>/nn/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:912, <span class="hljs-keyword">in</span> RobertaModel.forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)<br><span class="hljs-keyword">else</span>:<br>token_type_ids = torch.zeros(input_shape, <span class="hljs-attribute">dtype</span>=torch.long, <span class="hljs-attribute">device</span>=device)<br>--&gt; embedding_output =self.embeddings(<br><span class="hljs-attribute">input_ids</span>=input_ids,<br><span class="hljs-attribute">position_ids</span>=position_ids,<br><span class="hljs-attribute">token_type_ids</span>=token_type_ids,<br><span class="hljs-attribute">inputs_embeds</span>=inputs_embeds,<br><span class="hljs-attribute">past_key_values_length</span>=past_key_values_length,<br>)<br><span class="hljs-keyword">if</span> attention_mask isNone:<br>attention_mask = torch.ones((batch_size, seq_length + past_key_values_length), <span class="hljs-attribute">device</span>=device)<br><br>File ~/path/<span class="hljs-keyword">to</span>/nn/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:127, <span class="hljs-keyword">in</span> RobertaEmbeddings.forward(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)<br>embeddings = inputs_embeds + token_type_embeddings<br><span class="hljs-keyword">if</span> self.position_embedding_type ==<span class="hljs-string">&quot;absolute&quot;</span>:<br>--&gt; position_embeddings =self.position_embeddings(position_ids)<br>embeddings += position_embeddings<br>embeddings =self.LayerNorm(embeddings)<br><br>File ~/path/<span class="hljs-keyword">to</span>/nn/lib/python3.10/site-packages/torch/nn/functional.py:2551, <span class="hljs-keyword">in</span> embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)<br><span class="hljs-comment"># Note [embedding_renorm set_grad_enabled]</span><br><span class="hljs-comment"># <span class="hljs-doctag">XXX:</span> equivalent to</span><br><span class="hljs-comment"># with torch.no_grad():</span><br><span class="hljs-comment">#   torch.embedding_renorm_</span><br><span class="hljs-comment"># remove once script supports set_grad_enabled</span><br>no_grad_embedding_renorm(weight, input, max_norm, norm_type)<br>-&gt;  return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)IndexError: index out of range <span class="hljs-keyword">in</span> self<br></code></pre></td></tr></table></figure></blockquote><h3 id="报错了怎么办">报错了怎么办！</h3><p>女孩其实不懂怎么看报错信息、所以首先把所有报错（包括标准库的）全喂给ds并用最简单的问句【怎么解决】问了。乐的是ds虽然知道这是个索引超出范围问题，但一直在思维链里反复【不过在这里输入的序列长度是512，模型设置是512，应该没问题。】（刚解决此问题后得意忘形将对话记录删掉了啊啊啊啊让我试试能否重现），只能淡淡地又换了几个平台换了几种问问题方法（其实应该保留对话下来复盘一下应该怎么有效调教ai的但女孩全删掉了，女孩你这样是永远无法赚到卖课的钱的）。</p><p>问着问着虽然没能得到直接的解决方法，但从答案里还是可以知道：是position_ids这个变量里包含了超过模型允许的位置索引，从而导致了索引超出报错，可以着重看一下这句代码</p><blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">--&gt; <span class="hljs-number">127</span> position_embeddings =<span class="hljs-variable language_">self</span>.position_embeddings(position_ids)<br></code></pre></td></tr></table></figure></blockquote><p>我：之前用input_ids的时候也没遇到这个问题啊……但之前的序列长度由于分词器分词操作所以每句的长度不一而且都没有到512的，所以没出现这个问题。现在是每条序列的长度都为512，但模型的最大位置嵌入参数设置的也是512啊之前也没看到哪里说这个参数有什么设置的要求。出于懒、畏难情绪和对源代码的信任，女孩一开始并没有想着看看源代码在干嘛，而是尝试改了一下max_position_embeddings这个参数。结果为：改成513-- 继续报错，改成514 -- 不报错了。</p><p>？</p><p>到这里已经开始感觉：不会是索引起始和左闭右开吧……然后被叫去1on1了（女孩这是你的笔记还是日记？），谈话结束后摆烂一下午直至晚上才开始真正干活，终于回去看源代码这部分到底在干嘛，发现：</p><p>在<ahref="https://github.com/huggingface/transformers/blob/v4.48.2/src/transformers/models/roberta/modeling_roberta.py#L127">modeling_roberta.py</a>里找报错的第127行代码以及相关函数</p><blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-variable language_">self</span>.padding_idx = config.pad_token_id<br><span class="hljs-variable language_">self</span>.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size, padding_idx=<span class="hljs-variable language_">self</span>.padding_idx)<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self, input_ids=<span class="hljs-literal">None</span>, token_type_ids=<span class="hljs-literal">None</span>, position_ids=<span class="hljs-literal">None</span>, inputs_embeds=<span class="hljs-literal">None</span>, past_key_values_length=<span class="hljs-number">0</span></span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-keyword">if</span> position_ids <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">if</span> input_ids <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                <span class="hljs-comment"># Create the position ids from the input token ids. Any padded tokens remain padded.</span><br>                position_ids = create_position_ids_from_input_ids(input_ids, <span class="hljs-variable language_">self</span>.padding_idx, past_key_values_length)<br>            <span class="hljs-keyword">else</span>:<br>                position_ids = <span class="hljs-variable language_">self</span>.create_position_ids_from_inputs_embeds(inputs_embeds)<br><span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.position_embedding_type == <span class="hljs-string">&quot;absolute&quot;</span>:<br>--&gt; <span class="hljs-number">127</span>     position_embeddings = <span class="hljs-variable language_">self</span>.position_embeddings(position_ids)<br>            embeddings += position_embeddings<br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">create_position_ids_from_inputs_embeds</span>(<span class="hljs-params">self, inputs_embeds</span>):<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.</span><br><span class="hljs-string">Args:</span><br><span class="hljs-string">     inputs_embeds: torch.Tensor</span><br><span class="hljs-string"></span><br><span class="hljs-string">Returns: torch.Tensor</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        input_shape = inputs_embeds.size()[:-<span class="hljs-number">1</span>]<br>        sequence_length = input_shape[<span class="hljs-number">1</span>]<br><br>        position_ids = torch.arange(<br>            <span class="hljs-variable language_">self</span>.padding_idx + <span class="hljs-number">1</span>, sequence_length + <span class="hljs-variable language_">self</span>.padding_idx + <span class="hljs-number">1</span>, dtype=torch.long, device=inputs_embeds.device<br>        )<br>        <span class="hljs-keyword">return</span> position_ids.unsqueeze(<span class="hljs-number">0</span>).expand(input_shape)<br></code></pre></td></tr></table></figure></blockquote><p>既然第127代码导致了报错，那里面的position_ids是否也有问题？一开始我没有输入position_ids，所以RobertaEmbedding类里的函数create_position_ids_from_inputs_embeds生成了一个。后面ai又建议我自己创建一个position_ids(代码如下)，然后就没有报错了……怎么回事啊你们自己还和自己打架？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">position_ids = torch.arange(<span class="hljs-number">512</span>).expand(batch_size, -<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><h3 id="w-h-y">W H Y ?</h3><p>两种方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-variable language_">self</span>.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size, padding_idx=<span class="hljs-variable language_">self</span>.padding_idx) <span class="hljs-comment">#源代码</span><br>position_embeddings = <span class="hljs-variable language_">self</span>.position_embeddings(position_ids)<span class="hljs-comment"># ai建议</span><br></code></pre></td></tr></table></figure><table><thead><tr><th>源代码</th><th>ai建议</th></tr></thead><tbody><tr><td>我没有输入pad_token_id（因为确实不需要pad）<br />所以对于nn.Embedding来说padding_idx的默认值是0<br />由此这个函数生成的position_ids从1开始，[1,2,3,……,512]</td><td>从0开始，[0,1,2,……,511]</td></tr></tbody></table><p>aaaa果然是0based和1based你们两个eeeeeee</p><p>那为什么0开始的id可以在此成功运行而1开始的不行</p><h4 id="nn.embedding你又在干嘛">nn.Embedding你又在干嘛</h4><p>nn.embedding是pytorch的一个嵌入层，主要用于<strong>将离散的索引（如单词ID、类别ID）映射到连续的高维向量空间</strong>。其输入通常是类别型数据(categoricaldata)，比如nlp里的tokenid，推荐系统里的用户id等等。通俗来说就是一个可训练的查找表(lookuptable)，你给它一个整数索引，它返回这个索引对应的嵌入向量。</p><p>那么self.position_embeddings也就生成了一个表大小有512(max_position_embeddings)，维度有256(hidden_size)的表。这里的表大小不是word_embedding里面词汇表的大小，而是一条序列长度的大小（把词汇表里每个token替换成一条序列里每个位置）。下面调用这个表的时候，输入的索引就是position_ids。</p><p>embedding的表索引从0开始，所以源代码的id最后一个索引512在表里就没有对应的嵌入了，所以就indexerror了。</p><h4 id="又是啥意思">513又是啥意思</h4><p>但如果如上所述那为什么要改到514才正常……？难道不是513就应该正常了吗</p><p>女孩终于点开了解决问题最应该打开的issue页面，并搜索roberta positionids。然后发现roberta模型由于从fairseq那继承过来的时候没有改动原有的模型配置，其padding_idx的默认值其实是：1！恍然大悟的女孩终于发现了其实应该看看RobertaConfig的默认值……</p><p>ps：上面源代码里写的默认值其实是我想知道如果我不输入pad_token_id的时候默认值是什么，女孩直接去搜nn.embedding的默认值了，压根没有想到还有一个RobertaConfig……那么不输入pad_token_id时，源代码得到的position_ids其实是从2开始的。。</p><h3 id="所以怎么解决类似问题">所以怎么解决类似问题</h3><ol type="1"><li>自己输入position_ids</li><li>在config里把pad_token_id设置为-1(仅当你不用padtoken时。如果要用：自己构建position_ids/根据语料长度调整最长长度)</li></ol><h3 id="复盘">复盘</h3><p>小小问题竟浪费女孩这么多时间那么为了之后浪费的时间可以少一点我们认为需要复盘一下，但现在不想写，之后有空再更新吧(女孩你别一鸽就是永远)</p><h4 id="ps-再补充一下position-ids">ps 再补充一下position ids</h4><p>问了一下ds为什么根据inputs_embeds创建position_ids时（即函数<code>create_position_ids_from_inputs_embeds</code>）要考虑padding_idx。ds大思考一分多钟后答案总结如下：</p><blockquote><p>在Roberta模型中，创建位置IDs时考虑<code>padding_idx</code>的主要原因是为了避免生成的位置ID与嵌入层中的填充索引冲突，确保位置嵌入的有效性。具体原因如下：</p><ol type="1"><li><strong>避免位置嵌入的无效化</strong> ：位置嵌入层（<code>Embedding</code>）中的<code>padding_idx</code>对应的向量通常被设置为零且不参与训练。若位置ID等于<code>padding_idx</code>，该位置的位置嵌入会被固定为零，失去学习能力。通过从<code>padding_idx + 1</code>开始生成位置ID（如<code>padding_idx=0</code>时，ID从1开始），所有有效位置均获得非填充索引，确保位置嵌入被正常学习和使用。</li><li><strong>处理输入的不确定性</strong> ： 当直接提供<code>inputs_embeds</code>时，模型无法区分哪些位置是填充的（因嵌入已生成，原始输入信息丢失）。此时，函数<strong>默认所有位置有效</strong> ，生成连续的位置ID，并通过避开<code>padding_idx</code>确保即使存在填充，位置嵌入也不会错误引用填充索引。</li><li><strong>保持一致性</strong> ： 在标准流程中，模型会根据<code>input_ids</code>处理填充位置（如将填充token的位置ID设为0）。但当无法获取<code>input_ids</code>时，此函数采用保守策略，生成不与<code>padding_idx</code>冲突的ID，以模拟非填充场景，避免引入意外偏差。</li></ol></blockquote><p>所以确实和input_ids时的处理稍有不同，下面是id对应的函数<code>create_position_ids_from_input_ids</code>请看：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_position_ids_from_input_ids</span>(<span class="hljs-params">input_ids, padding_idx, past_key_values_length=<span class="hljs-number">0</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols</span><br><span class="hljs-string">    are ignored. This is modified from fairseq&#x27;s `utils.make_positions`.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        x: torch.Tensor x:</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns: torch.Tensor</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.</span><br>    mask = input_ids.ne(padding_idx).<span class="hljs-built_in">int</span>()<br>    incremental_indices = (torch.cumsum(mask, dim=<span class="hljs-number">1</span>).type_as(mask) + past_key_values_length) * mask<br>    <span class="hljs-keyword">return</span> incremental_indices.long() + padding_idx<br></code></pre></td></tr></table></figure><p>那么当padding_idx设置为3的时候，位置id就从4开始。那前面的0，1，2怎么办呢？ds说为了避免和填充索引冲突，这三个就不会被用作位置id了。</p><p>*不知道其他模型的实现里是否有这些问题，但请注意</p><h2 id="参考参考">参考参考</h2><p><ahref="https://github.com/huggingface/transformers/blob/v4.48.2/src/transformers/models/roberta/modeling_roberta.py#L1669">roberta源代码</a></p><p><ahref="https://github.com/huggingface/transformers/issues/10736#issuecomment-800175342">robertapad tokens</a></p>]]></content>
    
    
    <categories>
      
      <category>note</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>vcf2fasta</title>
    <link href="/2025/01/15/vcf2fasta/"/>
    <url>/2025/01/15/vcf2fasta/</url>
    
    <content type="html"><![CDATA[<h2 id="intro">intro</h2><blockquote><p>太好了！女孩只有在ddl迫近的时候才会想起来写这些、</p><p>其实有很多篇但不知为何有种一次就要搞篇大的但其实真的发了的只有一篇捡来的</p><p>所以又在捡！</p></blockquote><h2 id="tldr">TL;DR</h2><p>根据需要的fasta内容选软件，本文关心：</p><ol type="1"><li>上下游n bp</li><li>多态性位点所在元件整条序列</li></ol><p>不行的话找师兄师姐老师要序列吧、</p><h2 id="这是在干什么">这是在干什么</h2><p><strong>我们现在有一个存储了群体变异信息的vcf文件，但由于各种原因和下游应用，需要取出变异位点两侧的序列。</strong></p><p>作为存储变异信息的文件格式，vcf文件保存的信息可以说是位点信息。从位点信息到序列信息，需要做到的就是在检测变异时所用到的参考基因组里定位到相应位点，再按照后续对这个序列的操作目的来取包含这个位点的上下游序列(意思是你是想取这个多态性位点所在的基因/CDS或者什么特定的片段，还是你只是关心这个位点的邻居们都是些什么货色)。很简单的思路！不太确定自己写的工作量有多少，但既然别人都写过了为什么不拿、什么都写只会让你成为一个很厉害的写代码的我希望抱大腿的人！</p><h2 id="怎么做">怎么做</h2><h3 id="jvarkit----java-utilities-for-bioinformatics">jvarkit -- Javautilities for Bioinformatics</h3><p>在BioStar里绝望查找时找到的和当时我的需求最匹配的一个包，呵呵虽然之后还是又改了方向不再使用但还是献上我的感激之情！此包基于java开发，有超多功能简单总结这个包怎么用：</p><ol type="1"><li><p>先把参考基因组文件拿到！</p></li><li><p>jvarkit里的biostar251649jar包会在原vcf文件的基础上，在vcf的数据部分里的INFO部分加上两个字段：SEQ3_30和SEQ5_30(30是上下游序列的长度，可改)，输出相应的新vcf文件。</p></li><li><p>jvarkit里的bioalcidaejdk*jar包可以把这个新vcf文件里的两个新字段提取出来，和变异信息组成序列</p><p>*你以为这是脸滚键盘其实此包是java-based version of awk ofbioinformatics，bio和jdk好理解，Alcidae(海雀科)？因为海雀的英文是auk…just sooo random! 引用来自文档(女孩你还专门去看了…？)</p></li></ol><blockquote><p><strong>Why this name</strong></p><p>As 'bioalcidae' looks like an 'awk' for bioinformatics, we used '<ahref="https://en.wikipedia.org/wiki/Alcidae">Alcidae</a>', the taxonomicFamily of the '<a href="https://en.wikipedia.org/wiki/Auk">auk</a>'species.</p></blockquote><p>根据实际调试发现：此包对变异是SNP还是Indel，是双等位还是多等位都不敏感（下面例子可得）。</p><p>例子：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">## 得到的新vcf</span><br>$ java -jar dist/biostar251649.jar -n 10 -R tests/ref.fa tests/mutations.vcf<br><span class="hljs-comment">##INFO=&lt;ID=SEQ3_10,Number=1,Type=String,Description=&quot;Sequence on the 3&#x27; of mutation&quot;&gt;</span><br><span class="hljs-comment">##INFO=&lt;ID=SEQ5_10,Number=1,Type=String,Description=&quot;Sequence on the 5&#x27; of mutation&quot;&gt;</span><br>(...)<br><span class="hljs-comment">#CHROM  POS ID  REF ALT QUAL    FILTER  INFO    FORMAT  S1  S2  S3  S4</span><br>rotavirus   51  .   A   G   22.55   .   AC1=2;AF1=0.25;BQB=1;DP=944;DP4=849,0,93,0;FQ=23.7972;G3=0.75,0,0.25;HWE=0.033921;MQ=60;MQ0F=0;MQB=1;PV4=1,1,1,1;RPB=0.993129;SEQ3_10=GATGGTAAGC;SEQ5_10=TCTACTCAGC;SGB=-61.9012;VDB=3.53678e-05    GT:PL   0/0:0,255,134   0/0:0,255,127   0/0:0,255,137   1/1:70,255,0<br>rotavirus   91  .   A   T   5.45    .   AC1=1;AF1=0.124963;BQB=0.951201;DP=1359;DP4=1134,0,225,0;FQ=5.8713;MQ=60;MQ0F=0;MQB=1;PV4=1,4.80825e-05,1,1;RPB=0.0393173;SEQ3_10=GTTGTTGCTG;SEQ5_10=TTGAAGCTGC;SGB=-369.163;VDB=0.313337   GT:PL   0/0:0,255,133   0/1:40,0,31 0/0:0,255,134   0/0:0,255,82<br><br><span class="hljs-comment">## 取序列</span><br>java -jar dist/bioalcidaejdk.jar -F VCF -e <span class="hljs-string">&#x27;stream().forEach(V-&gt;println(&quot;&gt;&quot;+V.getContig()+&quot;:&quot;+V.getStart()+&quot;\n&quot;+V.getAttribute(&quot;SEQ5_20&quot;)+&quot;[&quot;+V.getAlleles().stream().map(A-&gt;A.getDisplayString()).collect(Collectors.joining(&quot;/&quot;))+&quot;]&quot;+V.getAttribute(&quot;SEQ3_20&quot;)));&#x27;</span><br><br>&gt;rotavirus:51<br>TGGTCGATTGCTCTATTGAA[A/G]AATTTCCATTGATGGCTAAA <br><span class="hljs-comment"># 这里会把包括参考基因型和变异基因型都列在[]里，所以说对snp/indel，双/多等位不会很敏感</span><br></code></pre></td></tr></table></figure><p>详解第二步代码的-e(来自作者在biostars 334253里的回答)</p><blockquote><p><code>stream()</code> convert to a stream of variant</p><p><code>forEach(V-&gt;println("&gt;"</code> for each variant startwriting the fasta header</p><p><code>V.getContig()+":"+V.getStart()+"\n"</code> write the fastaheader</p><p><code>V.getAttribute("SEQ5_20")</code> write 5' seq</p><p><code>["+V.getAlleles().stream().map(A-&gt;A.getDisplayString()).collect(Collectors.joining("/"))+"]"</code>write all alleles separated with '/'</p><p><code>V.getAttribute("SEQ3_20")))</code> write 3' seq</p></blockquote><p>我自己批量处理vcf的一段代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><code class="hljs bash">/#!/bin/bash<br><span class="hljs-comment">#SBATCH --job-name=          # 作业名称</span><br><span class="hljs-comment">#SBATCH --output=        # 标准输出文件</span><br><span class="hljs-comment">#SBATCH --error=         # 标准错误文件</span><br><span class="hljs-comment">#SBATCH --time=                     # 运行时间（格式：HH:MM:SS）</span><br><span class="hljs-comment">#SBATCH --ntasks=1                          # 分配任务数</span><br><span class="hljs-comment">#SBATCH --cpus-per-task=4                   # 每个任务分配的CPU数量（根据需求调整）</span><br><span class="hljs-comment">#SBATCH --mem=16G                           # 分配内存（根据需求调整）</span><br><br><br><span class="hljs-comment"># 设置脚本在遇到错误时停止</span><br><span class="hljs-built_in">set</span> -e<br><br><span class="hljs-comment"># 样品文件路径</span><br>SAMPLE_FILE=<span class="hljs-string">&quot;/path/to/sample/file&quot;</span><br><br><span class="hljs-comment"># VCF 输入文件路径</span><br>VCF_INPUT=<span class="hljs-string">&quot;/path/to/vcf&quot;</span><br><br><span class="hljs-comment"># 基因组参考序列路径</span><br>REFERENCE_GENOME=<span class="hljs-string">&quot;/path/to/fna&quot;</span><br><br><span class="hljs-comment"># JVarkit 工具路径</span><br>JVARKIT_JAR=<span class="hljs-string">&quot;/path/to/jvarkit.jar&quot;</span><br><br><span class="hljs-comment"># 输出目录（可选，建议统一输出到一个目录）</span><br>OUTPUT_DIR=<span class="hljs-string">&quot;/path/to/output/dir&quot;</span><br><span class="hljs-built_in">mkdir</span> -p <span class="hljs-string">&quot;<span class="hljs-variable">$OUTPUT_DIR</span>&quot;</span><br><br><span class="hljs-comment"># 读取每个样品并处理</span><br><span class="hljs-keyword">while</span> IFS= <span class="hljs-built_in">read</span> -r SAMPLE || [[ -n <span class="hljs-string">&quot;<span class="hljs-variable">$SAMPLE</span>&quot;</span> ]]; <span class="hljs-keyword">do</span><br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;开始处理样品: <span class="hljs-variable">$SAMPLE</span>&quot;</span><br><br>    <span class="hljs-comment"># 步骤 1: 使用 bcftools 提取样品 VCF</span><br>    bcftools view -Ov -s <span class="hljs-string">&quot;<span class="hljs-variable">$SAMPLE</span>&quot;</span> <span class="hljs-string">&quot;<span class="hljs-variable">$VCF_INPUT</span>&quot;</span> -o <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;OUTPUT_DIR&#125;</span>/<span class="hljs-variable">$&#123;SAMPLE&#125;</span>.vcf&quot;</span><br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;生成 <span class="hljs-variable">$&#123;SAMPLE&#125;</span>.vcf&quot;</span><br><br>    <span class="hljs-comment"># 步骤 2: 使用 GATK 选择变异 (主要是想取每个样品的变异)</span><br>    gatk SelectVariants -V <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;OUTPUT_DIR&#125;</span>/<span class="hljs-variable">$&#123;SAMPLE&#125;</span>.vcf&quot;</span> --exclude-non-variants -O <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;OUTPUT_DIR&#125;</span>/<span class="hljs-variable">$&#123;SAMPLE&#125;</span>_variant.vcf&quot;</span><br>    <span class="hljs-comment">#echo &quot;生成 $&#123;SAMPLE&#125;_variant.vcf&quot;</span><br><br>    <span class="hljs-comment"># 步骤 3: 使用 JVarkit 提取 flanking regions</span><br>    java -jar <span class="hljs-string">&quot;<span class="hljs-variable">$JVARKIT_JAR</span>&quot;</span> biostar251649 \<br>        -R <span class="hljs-string">&quot;<span class="hljs-variable">$REFERENCE_GENOME</span>&quot;</span> \<br>        <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;OUTPUT_DIR&#125;</span>/<span class="hljs-variable">$&#123;SAMPLE&#125;</span>.vcf&quot;</span> -n 50 &gt; <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;OUTPUT_DIR&#125;</span>/<span class="hljs-variable">$&#123;SAMPLE&#125;</span>_flank.vcf&quot;</span><br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;生成 <span class="hljs-variable">$&#123;SAMPLE&#125;</span>_flank.vcf&quot;</span><br><br>    <span class="hljs-comment"># 步骤 4: 使用 JVarkit 生成 FASTA 文件</span><br>    <span class="hljs-comment"># 如果改了上下游长度，这里也记得改</span><br>    java -jar <span class="hljs-string">&quot;<span class="hljs-variable">$JVARKIT_JAR</span>&quot;</span> bioalcidaejdk \<br>        -F VCF -e <span class="hljs-string">&#x27;stream().forEach(V-&gt;println(&quot;&gt;&quot;+V.getContig()+&quot;:&quot;+V.getStart()+&quot;\n&quot;+V.getAttribute(&quot;SEQ5_50&quot;)+&quot;[&quot;+V.getAlleles().stream().map(A-&gt;A.getDisplayString()).collect(Collectors.joining(&quot;/&quot;))+&quot;]&quot;+V.getAttribute(&quot;SEQ3_50&quot;)));&#x27;</span> \<br>        <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;OUTPUT_DIR&#125;</span>/<span class="hljs-variable">$&#123;SAMPLE&#125;</span>_flank.vcf&quot;</span> &gt; <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;OUTPUT_DIR&#125;</span>/<span class="hljs-variable">$&#123;SAMPLE&#125;</span>_flank.fasta&quot;</span><br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;生成 <span class="hljs-variable">$&#123;SAMPLE&#125;</span>_flank.fasta&quot;</span><br><br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;完成处理样品: <span class="hljs-variable">$SAMPLE</span>&quot;</span><br>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;-----------------------------------------&quot;</span><br><br><span class="hljs-keyword">done</span> &lt; <span class="hljs-string">&quot;<span class="hljs-variable">$SAMPLE_FILE</span>&quot;</span><br><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;所有样品处理完成！&quot;</span><br></code></pre></td></tr></table></figure><h3 id="vcf2fasta">vcf2fasta</h3><p>我猜这个应该是之前用得比较多的一个……？但不知道之前大家用来干什么。和上面的包的主要区别是这个包会根据gff文件里面的基因组特征提取序列，所以这个包的序列提取是以基因组特征为主的，和上面以多态性位点为主不同。</p><p>翻找自己的文件发现早就把使用此包的代码删掉了，因为当时用了之后结果……【前情提要：当时的需求是取所有样品某个基因的序列，在有测序数据的前提下其实可以用组装数据来做但我没有！所以就先试着根据有的群体vcf文件和参考序列来取了】不能说是错吧，可能和用的gff文件里面注释有关系，取出来的蛋白质编码基因不是起始密码子开头，影响了后续的计算！回首过往觉得如果要解决的话应该是回头确认gff文件对目标基因的注释到底是从哪开始，对应的具体序列又是什么。虽然软件对CDS的inframe是有参数设定的，但我需要整个基因！当时因为太赶了最后通过最简单也是最难的【问师兄要组装好的序列】解决了呵呵。所以后续如果还需要使用请注意。</p><p>怎么用：</p><ol type="1"><li><p>准备参考序列，需要参考基因组的fa文件和gff文件。参考基因组的fa文件需要用samtools进行index；如果你的gff文件非常有个人特色的话，去github看看符不符合软件的要求。vcf文件也要index，但用tabix。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">samtools faidx ref.fa<br>bgzip my_vcf_file.vcf<br>tabix my_vcf_file.vcf.gz<br></code></pre></td></tr></table></figure></li><li><p>下载本体、pysam和art包</p></li><li><p>最简单的用法，其他参数按-h或者看github吧：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">python vcf2fasta.py -f genome.fas -v variants.vcf.gz -g intervals.gff -e CDS<br></code></pre></td></tr></table></figure></li><li><p>设定了-e CDS之后会输出所有的CDS，，希望之后能有用</p></li></ol><h2 id="上面的链接">上面的链接🔗</h2><p><a href="https://github.com/lindenb/jvarkit">jvarkit_github</a></p><p><ahref="https://jvarkit.readthedocs.io/en/latest/">jvarkit_document</a></p><p><ahref="https://github.com/santiagosnchez/vcf2fasta">vcf2fasta</a></p><p>BioStars的帖子</p><p><a href="https://www.biostars.org/p/251649/">251649</a></p><p><a href="https://www.biostars.org/p/334253/">334253</a></p><h2 id="结束">结束、</h2><p>此地的一个好处是写正经笔记的同时女孩还是不由自主散发出一种absent-minded的语言风格，但如果你的名字不是没有人的话你不会在意。</p>]]></content>
    
    
    <categories>
      
      <category>note</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>为什么python输出在out文件里不显示啊！！</title>
    <link href="/2024/12/10/flush_slurm/"/>
    <url>/2024/12/10/flush_slurm/</url>
    
    <content type="html"><![CDATA[<h2 id="此竟然为第一篇">此竟然为第一篇、</h2><blockquote><p>其实断断续续有五篇草稿但最后还是先把捡来的东西发了、、、</p></blockquote><p>为什么投完任务之后我的进度输出不显示在out文件里啊！！——来自：要处理多个样品本来以为一晚上肯定结束早上上工输入squeue后发现仍遥遥无期终于在18个小时后认为等待是高风险【主要还是老师降临视察后感到大祸临头】于是加了两条输出意图监控进度但发现1小时过去out文件干净得如同脑子的一位</p><p>被光盘头糊弄两个来回后发现咕狗给我的答案才是正确的，实践后痛斥光盘头得到以下总结，之后请留意：</p><h2 id="tldr">TL;DR</h2><p>用python -u urscript.py</p><h2 id="是缓冲区">是缓冲区！</h2><p>Python 的 <code>print()</code>函数将内容输出到标准输出（通常是终端或文件），但是它并不是立即将内容写入屏幕或文件中。为了提高效率，Python会将输出内容缓存起来，然后在适当的时候（如缓冲区满时）才将这些内容一次性地输出。</p><blockquote><p>输出缓冲区的三种方式：</p><ol type="1"><li><strong>行缓冲</strong> ：当输出中包含换行符（例如<code>print("Hello")</code>）时，Python会自动刷新缓冲区并输出内容。通常在交互式环境（如终端）中，<code>print()</code>会及时显示。</li><li><strong>全缓冲</strong> ：对于文件和重定向到文件的标准输出，Python默认会使用全缓冲模式，即当缓冲区满时才会写入文件。这意味着如果你在文件中输出，直到缓冲区满或者程序结束，才会将内容写入文件。</li><li><strong>无缓冲</strong> ：无缓冲模式下，<code>print()</code>函数会立即将输出内容写入文件或显示到终端。你可以通过设置 Python的输出流为无缓冲来避免缓冲的问题。</li></ol></blockquote><h2 id="缓冲区为什么导致此问题">缓冲区为什么导致此问题</h2><p>在提交作业时（比如使用 <code>sbatch</code>），Slurm会将标准输出和标准错误输出重定向到文件中。如果 Python使用了缓冲输出，而脚本没有结束或者没有写入换行符，缓冲区的内容可能不会被及时写入输出文件。这样，你就无法在日志文件中看到<code>print</code> 输出。</p><h2id="解决办法虽然都是一回事但仍然摆出三条like-why因为是笔记">解决办法（虽然都是一回事但仍然摆出三条，likewhy（因为是笔记</h2><ol type="1"><li>显式刷新缓冲区： 可以通过在 <code>print()</code> 语句后面添加flush=True 来强制 Python 在每次输出时刷新缓冲区。例如：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;sample_name&#125;</span> processed&quot;</span>, flush=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>这样确保每次<code>print()</code>输出都会刷新缓冲区，从而输出到out文件里</p><ol start="2" type="1"><li>用 sys.stdout.flush()： 也可以显式地使用<code>sys.stdout.flush()</code>来刷新缓冲区。这可以在脚本的任何地方调用，以确保输出被及时刷新到标准输出。例如：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> sys<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;sample_name&#125;</span> processed&quot;</span>)<br>sys.stdout.flush()  <span class="hljs-comment"># 强制刷新缓冲区</span><br></code></pre></td></tr></table></figure><ol start="3" type="1"><li>禁用缓冲：如果要在所有输出中都禁用缓冲，则可以直接在运行脚本时使用-u参数，让python以无缓冲模式运行</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">python -u my_script.py<br></code></pre></td></tr></table></figure><ol start="4" type="1"><li>使用日志模块：<em>(怎么回事不是只有三条吗！)(因为和上面三个不一样)</em>可以考虑用python的<code>logging</code>模块<em>(没用过)</em>，此模块允许你记录调试、信息、警告、错误和致命错误<em>(wow)</em>等不同级别日志，可以配置其使得输出立即写入文件/定期刷新</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> logging<br><br>logging.basicConfig(level=logging.DEBUG, <span class="hljs-built_in">format</span>=<span class="hljs-string">&#x27;%(asctime)s - %(message)s&#x27;</span>, handlers=[logging.StreamHandler(), logging.FileHandler(<span class="hljs-string">&#x27;job_output.log&#x27;</span>)])<br>logging.debug(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;sample_name&#125;</span> processed&quot;</span>)<br><span class="hljs-comment"># 谁看懂了、</span><br></code></pre></td></tr></table></figure><h2 id="参考链接">参考链接🔗</h2><p><ahref="https://blog.csdn.net/MissShihong/article/details/107862293">同病相怜文</a></p><p>&amp;光盘头老师</p><h2 id="结束-退朝">结束 退朝</h2>]]></content>
    
    
    <categories>
      
      <category>note</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>hello world</title>
    <link href="/2024/10/05/hello-world/"/>
    <url>/2024/10/05/hello-world/</url>
    
    <content type="html"><![CDATA[<h3 id="mood">mood:</h3><p><em>陷入一种小时候拿到一个漂亮本子却一时不知道要在上面写什么的期待和踌躇感。</em></p><blockquote><p>sorry but 本人的所有开头大都如此潦草</p></blockquote><p>人为给自己创造了一个新需求，随便放点。</p><p>主要是一些笔记【hopeso】，这位小姐最好不要把这里也搞成自己的ego和坏情绪随意发射地、、</p>]]></content>
    
    
    <categories>
      
      <category>random</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
