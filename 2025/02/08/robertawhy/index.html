

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/icons/Target.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="ph-bear">
  <meta name="keywords" content="">
  
    <meta name="description" content="intro  roberta模型为何这样  问题 普遍的nlp构建模型流程（可能吧因为我不做自然语言）（DNA难道不是一种真正的自然语言……？）：得到待训练的文本 -- 对文本做些预处理 -- 选择并训练适合的分词器 -- 用分词器对处理好的文本进行分词，得到词索引 -- 组织成可以批量输入到模型里的数据结构，开始训练。但我现在的科学问题已经进到了需要用奇技淫巧（有必要这么形容吗）来绕道的胡同，不">
<meta property="og:type" content="article">
<meta property="og:title" content="roberta模型你在干嘛">
<meta property="og:url" content="https://patchaven.github.io/2025/02/08/robertawhy/index.html">
<meta property="og:site_name" content="patchaven">
<meta property="og:description" content="intro  roberta模型为何这样  问题 普遍的nlp构建模型流程（可能吧因为我不做自然语言）（DNA难道不是一种真正的自然语言……？）：得到待训练的文本 -- 对文本做些预处理 -- 选择并训练适合的分词器 -- 用分词器对处理好的文本进行分词，得到词索引 -- 组织成可以批量输入到模型里的数据结构，开始训练。但我现在的科学问题已经进到了需要用奇技淫巧（有必要这么形容吗）来绕道的胡同，不">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-02-07T16:00:00.000Z">
<meta property="article:modified_time" content="2025-02-08T13:28:20.221Z">
<meta property="article:author" content="ph-bear">
<meta name="twitter:card" content="summary_large_image">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>roberta模型你在干嘛 - patchaven</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"patchaven.github.io","root":"/","version":"1.9.8","typing":{"enable":false,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":"63fd1025b507ed16c095540ece0646cf","google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  
    <!-- Baidu Analytics -->
    <script async>
      if (!Fluid.ctx.dnt) {
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?63fd1025b507ed16c095540ece0646cf";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
      }
    </script>
  

  

  

  

  



  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 35vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>patchaven</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/" target="_self">
                <i class="iconfont icon-link-fill"></i>
                <span>链接</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/bg/show.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.6)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle">roberta模型你在干嘛</span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-02-08 00:00" pubdate>
          2025年2月8日 凌晨
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          2.8k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          24 分钟
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        

      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">roberta模型你在干嘛</h1>
            
            
              <div class="markdown-body">
                
                <h2 id="intro">intro</h2>
<blockquote>
<p>roberta模型为何这样</p>
</blockquote>
<h2 id="问题">问题</h2>
<p>普遍的nlp构建模型流程（可能吧因为我不做自然语言）（DNA难道不是一种真正的自然语言……？）：得到待训练的文本
-- 对文本做些预处理 -- 选择并训练适合的分词器 --
用分词器对处理好的文本进行分词，得到词索引 --
组织成可以批量输入到模型里的数据结构，开始训练。但我现在的科学问题已经进到了需要用奇技淫巧（有必要这么形容吗）来绕道的胡同，不知是福是祸啊！所以现在我想做的是：绕过抱抱脸里已经实现好的模型里已有embedding层，自己先给一个起始的嵌入。虽然不知道这对此科学问题到底增益几何，但技术问题已堂堂袭来！</p>
<h3 id="复现问题">复现问题</h3>
<p>对原始数据操作一番后，先取了一个batch的嵌入试试能不能跑通，其形状为
<code>torch.Size([32, 512, 4])</code>。</p>
<p>根据roberta模型说明页面里面对inputs_embeds的说明：</p>
<blockquote>
<p>(<code>torch.FloatTensor</code> of shape
<code>(batch_size, sequence_length, hidden_size)</code>,
<em>optional</em> ) — Optionally, instead of passing
<code>input_ids</code> you can choose to directly pass an embedded
representation. This is useful if you want more control over how to
convert <code>input_ids</code> indices into associated vectors than the
model’s internal embedding lookup matrix.</p>
</blockquote>
<p>模型的参数设置：</p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">roberta_config = RobertaConfig(<br>		vocab_size=<span class="hljs-number">20</span>,  <span class="hljs-comment">#没想好，先占个位</span><br>		hidden_size=<span class="hljs-number">256</span>,  <br>		num_hidden_layers=<span class="hljs-number">12</span>,  <br>		num_attention_heads=<span class="hljs-number">8</span>,  <br>		intermediate_size=<span class="hljs-number">1024</span>,  <br>		max_position_embeddings=<span class="hljs-number">512</span>,    )<br></code></pre></td></tr></table></figure>
</blockquote>
<p>既然输入的张量最后一个维度要和模型的隐藏层维度相同，那么我就用了一个nn.linear把4维升到256维，得到
<code>torch.Size([32, 512, 256])</code>的张量。</p>
<p>将这个张量输入到roberta模型后报错（跳过标准库，截取部分报错信息，去掉行号）：</p>
<blockquote>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs vim">---------------------------------------------------------------------------IndexError                                Traceback (most recent <span class="hljs-keyword">call</span> <span class="hljs-keyword">last</span>)<br>Cell **In[<span class="hljs-number">21</span>], **<span class="hljs-built_in">line</span> <span class="hljs-number">1</span><br>**----&gt; **<span class="hljs-number">1</span> out1 = roberta_lm_model(inputs_embeds=out, output_hidden_states=True,position_ids=position_ids)File ~/path/<span class="hljs-keyword">to</span>/<span class="hljs-keyword">nn</span>/lib/<span class="hljs-keyword">python3</span>.<span class="hljs-number">10</span>/site-packages/transformers/models/roberta/modeling_roberta.<span class="hljs-keyword">py</span>:<span class="hljs-number">1204</span>, in RobertaForMaskedLM.forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)labels (torch.LongTensor of shape (batch_size, sequence_length), optional):<br>Labels <span class="hljs-keyword">for</span> computing the masked <span class="hljs-keyword">language</span> modeling loss. Indices should <span class="hljs-keyword">be</span> in `[-<span class="hljs-number">100</span>, <span class="hljs-number">0</span>, ...,<br>(...)<br>Used <span class="hljs-keyword">to</span> <span class="hljs-keyword">hide</span> legacy arguments that have been deprecated.<br><span class="hljs-string">&quot;&quot;</span><span class="hljs-comment">&quot;</span><br>return_dict = return_dict <span class="hljs-keyword">if</span> return_dict isnotNoneelseself.config.use_return_dict<br>-&gt;  outputs =self.roberta(<br>input_ids,<br>attention_mask=attention_mask,<br>token_type_ids=token_type_ids,<br>position_ids=position_ids,<br>head_mask=head_mask,<br>inputs_embeds=inputs_embeds,<br>encoder_hidden_states=encoder_hidden_states,<br>encoder_attention_mask=encoder_attention_mask,<br>output_attentions=output_attentions,<br>output_hidden_states=output_hidden_states,<br>return_dict=return_dict,<br>)<br>sequence_output = outputs[<span class="hljs-number">0</span>]<br>prediction_scores =self.lm_head(sequence_output)File ~/path/<span class="hljs-keyword">to</span>/<span class="hljs-keyword">nn</span>/lib/<span class="hljs-keyword">python3</span>.<span class="hljs-number">10</span>/site-packages/transformers/models/roberta/modeling_roberta.<span class="hljs-keyword">py</span>:<span class="hljs-number">912</span>, in RobertaModel.forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)<br><span class="hljs-keyword">else</span>:<br>token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)<br>--&gt; embedding_output =self.embeddings(<br>input_ids=input_ids,<br>position_ids=position_ids,<br>token_type_ids=token_type_ids,<br>inputs_embeds=inputs_embeds,<br>past_key_values_length=past_key_values_length,<br>)<br><span class="hljs-keyword">if</span> attention_mask isNone:<br>attention_mask = torch.ones((batch_size, seq_length + past_key_values_length), device=device)File ~/path/<span class="hljs-keyword">to</span>/<span class="hljs-keyword">nn</span>/lib/<span class="hljs-keyword">python3</span>.<span class="hljs-number">10</span>/site-packages/transformers/models/roberta/modeling_roberta.<span class="hljs-keyword">py</span>:<span class="hljs-number">127</span>, in RobertaEmbeddings.forward(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)<br>embeddings = inputs_embeds + token_type_embeddings<br><span class="hljs-keyword">if</span> self.position_embedding_type ==<span class="hljs-string">&quot;absolute&quot;</span>:<br>position_embeddings =self.position_embeddings(position_ids)<br>embeddings += position_embeddings<br>embeddings =self.LayerNorm(embeddings)File ~/path/<span class="hljs-keyword">to</span>/<span class="hljs-keyword">nn</span>/lib/<span class="hljs-keyword">python3</span>.<span class="hljs-number">10</span>/site-packages/torch/<span class="hljs-keyword">nn</span>/functional.<span class="hljs-keyword">py</span>:<span class="hljs-number">2551</span>, in embedding(<span class="hljs-built_in">input</span>, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)<br># Note [embedding_renorm set_grad_enabled]<br># XXX: equivalent <span class="hljs-keyword">to</span><br># with torch.no_grad():<br>#   torch.embedding_renorm_<br># <span class="hljs-built_in">remove</span> once script supports set_grad_enabled<br>no_grad_embedding_renorm(weight, <span class="hljs-built_in">input</span>, max_norm, norm_type)<br>-&gt;  <span class="hljs-keyword">return</span> torch.embedding(weight, <span class="hljs-built_in">input</span>, padding_idx, scale_grad_by_freq, sparse)IndexError: <span class="hljs-built_in">index</span> out of <span class="hljs-built_in">range</span> in self<br></code></pre></td></tr></table></figure>
</blockquote>
<h3 id="报错了怎么办">报错了怎么办！</h3>
<p>女孩其实不懂怎么看报错信息、所以首先把所有报错（包括标准库的）全喂给ds并用最简单的问句【怎么解决】问了。乐的是ds虽然知道这是个索引超出范围问题，但一直在思维链里反复【不过在这里输入的序列长度是512，模型设置是512，应该没问题。】（刚解决此问题后得意忘形将对话记录删掉了啊啊啊啊让我试试能否重现），只能淡淡地又换了几个平台换了几种问问题方法（其实应该保留对话下来复盘一下应该怎么有效调教ai的但女孩全删掉了，女孩你这样是永远无法赚到卖课的钱的）。</p>
<p>问着问着虽然没能得到直接的解决方法，但从答案里还是可以知道：是position_ids这个变量里包含了超过模型允许的位置索引，从而导致了索引超出报错，可以着重看一下这句代码</p>
<blockquote>
<p><strong>--&gt; </strong><a
target="_blank" rel="noopener" href="https://vscode-remote+ssh-002dremote-002b172-002e22-002e148-002e93.vscode-resource.vscode-cdn.net/public4/home/lihb/omics_trait/snp_matrix_pretrain/code/model/model_mh_linear/~/miniconda3/envs/nn/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:127">127</a>
position_embeddings =self.position_embeddings(position_ids)</p>
</blockquote>
<p>我：之前用input_ids的时候也没遇到这个问题啊……但之前的序列长度由于分词器分词操作所以每句的长度不一而且都没有到512的，所以没出现这个问题。现在是每条序列的长度都为512，但模型的最大位置嵌入参数设置的也是512啊之前也没看到哪里说这个参数有什么设置的要求。出于懒、畏难情绪和对源代码的信任，女孩一开始并没有想着看看源代码在干嘛，而是尝试改了一下max_position_embeddings这个参数。结果为：改成513
-- 继续报错，改成514 -- 不报错了。</p>
<p>？</p>
<p>到这里已经开始感觉：不会是索引起始和左闭右开吧……然后被叫去1on1了（女孩这是你的笔记还是日记？），谈话结束后摆烂一下午直至晚上才开始真正干活，终于回去看源代码这部分到底在干嘛，发现：</p>
<p>在<a
target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/v4.48.2/src/transformers/models/roberta/modeling_roberta.py#L127">modeling_roberta.py</a>里找报错的第127行代码</p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-variable language_">self</span>.padding_idx = config.pad_token_id<br><span class="hljs-variable language_">self</span>.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size, padding_idx=<span class="hljs-variable language_">self</span>.padding_idx)<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self, input_ids=<span class="hljs-literal">None</span>, token_type_ids=<span class="hljs-literal">None</span>, position_ids=<span class="hljs-literal">None</span>, inputs_embeds=<span class="hljs-literal">None</span>, past_key_values_length=<span class="hljs-number">0</span></span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-keyword">if</span> position_ids <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">if</span> input_ids <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                <span class="hljs-comment"># Create the position ids from the input token ids. Any padded tokens remain padded.</span><br>                position_ids = create_position_ids_from_input_ids(input_ids, <span class="hljs-variable language_">self</span>.padding_idx, past_key_values_length)<br>            <span class="hljs-keyword">else</span>:<br>                position_ids = <span class="hljs-variable language_">self</span>.create_position_ids_from_inputs_embeds(inputs_embeds)<br><span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.position_embedding_type == <span class="hljs-string">&quot;absolute&quot;</span>:<br>--&gt; <span class="hljs-number">127</span>     position_embeddings = <span class="hljs-variable language_">self</span>.position_embeddings(position_ids)<br>            embeddings += position_embeddings<br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">create_position_ids_from_inputs_embeds</span>(<span class="hljs-params">self, inputs_embeds</span>):<br>	<span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">	We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.</span><br><span class="hljs-string">	Args:</span><br><span class="hljs-string">	     inputs_embeds: torch.Tensor</span><br><span class="hljs-string"></span><br><span class="hljs-string">	Returns: torch.Tensor</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        input_shape = inputs_embeds.size()[:-<span class="hljs-number">1</span>]<br>        sequence_length = input_shape[<span class="hljs-number">1</span>]<br><br>        position_ids = torch.arange(<br>            <span class="hljs-variable language_">self</span>.padding_idx + <span class="hljs-number">1</span>, sequence_length + <span class="hljs-variable language_">self</span>.padding_idx + <span class="hljs-number">1</span>, dtype=torch.long, device=inputs_embeds.device<br>        )<br>        <span class="hljs-keyword">return</span> position_ids.unsqueeze(<span class="hljs-number">0</span>).expand(input_shape)<br></code></pre></td></tr></table></figure>
</blockquote>
<p>既然第127代码导致了报错，那里面的position_ids是否也有问题？一开始我没有输入position_ids，所以RobertaEmbedding类里的函数create_position_ids_from_inputs_embeds生成了一个。后面ai又建议我自己创建一个position_ids(代码如下)，然后就没有报错了……怎么回事啊你们自己还和自己打架？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">position_ids = torch.arange(<span class="hljs-number">512</span>).expand(batch_size, -<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure>
<h3 id="w-h-y">W H Y ?</h3>
<table>

<thead>
<tr>
<th>源代码</th>
<th>ai</th>
</tr>
</thead>
<tbody>
<tr>
<td>我没有输入pad_token_id（因为确实不需要pad），所以padding_idx的默认值是0，由此这个函数生成的position_ids是[1,2,3,……,512]</td>
<td>[0,1,2,……,511]，从0开始</td>
</tr>
</tbody>
</table>
<p>aaaa果然是0based和1based你们两个eeeeeee</p>
<p>那为什么0开始的id可以在此成功运行而1开始的不行</p>
<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs lua"><span class="hljs-built_in">self</span>.position_embeddings = nn.Embedding(<span class="hljs-built_in">config</span>.max_position_embeddings, <span class="hljs-built_in">config</span>.hidden_size, padding_idx=<span class="hljs-built_in">self</span>.padding_idx)<br>position_embeddings = <span class="hljs-built_in">self</span>.position_embeddings(position_ids) <br></code></pre></td></tr></table></figure>
<h4 id="nn.embedding在干嘛">nn.Embedding在干嘛</h4>
<p>nn.embedding是pytorch的一个嵌入层，主要用于<strong>将离散的索引（如单词
ID、类别
ID）映射到连续的高维向量空间</strong>。其输入通常是类别型数据(categorical
data)，比如nlp里的token
id，推荐系统里的用户id等等。通俗来说就是一个可训练的查找表(lookup
table)，你给它一个整数索引，它返回这个索引对应的嵌入向量。</p>
<p>那么self.position_embeddings也就生成了一个表大小有512(max_position_embeddings)，维度有256(hidden_size)的表。这里的表大小不是word_embedding里面词汇表的大小，而是一条序列长度的大小（把词汇表里每个token替换成一条序列里每个位置）。下面调用这个表的时候，输入的索引就是position_ids。</p>
<p>embedding的表索引从0开始，所以源代码的id最后一个索引512在表里就没有对应的嵌入了，所以就index
error了。</p>
<h4 id="又是啥意思">513又是啥意思</h4>
<p>但如果如上所述那为什么要改到514才正常……？难道不是513就应该正常了吗</p>
<p>女孩终于点开了解决问题最应该打开的issue页面，并搜索roberta position
ids。然后发现roberta模型由于从fairseq那继承过来的时候没有改动原有的模型配置，其padding_idx的默认值其实是：1！</p>
<p>上面源代码里写的默认值其实是我想知道如果我不输入pad_token_id的时候默认值是什么，女孩直接去搜nn.embedding的默认值了，压根没有想到还有一个RobertaConfig……
那么不输入pad_token_id时，源代码得到的position_ids其实是从2开始的。。</p>
<h3 id="所以怎么解决类似问题">所以怎么解决类似问题</h3>
<ol type="1">
<li>自己输入position_ids</li>
<li>在config里把pad_token_id设置为-1(当你不用pad
token时，如果要用感觉又有一些不同)</li>
</ol>
<h3 id="复盘">复盘</h3>
<p>小小问题竟浪费女孩这么多时间那么为了之后浪费的时间可以少一点我们认为需要复盘一下，但现在不想写，之后有空再更新吧(女孩你别一鸽就是永远)</p>
<h4 id="ps-再补充一下position-ids">ps 再补充一下position ids</h4>
<p>问了一下ds为什么创建position_ids时要考虑padding_idx。ds大思考一分多钟后答案总结如下：</p>
<blockquote>
<p>在Roberta模型中，创建位置IDs时考虑
<code>padding_idx</code>的主要原因是为了避免生成的位置ID与嵌入层中的填充索引冲突，确保位置嵌入的有效性。具体原因如下：</p>
<ol type="1">
<li><strong>避免位置嵌入的无效化</strong> ：
位置嵌入层（<code>Embedding</code>）中的
<code>padding_idx</code>对应的向量通常被设置为零且不参与训练。若位置ID等于
<code>padding_idx</code>，该位置的位置嵌入会被固定为零，失去学习能力。通过从
<code>padding_idx + 1</code>开始生成位置ID（如
<code>padding_idx=0</code>时，ID从1开始），所有有效位置均获得非填充索引，确保位置嵌入被正常学习和使用。</li>
<li><strong>处理输入的不确定性</strong> ： 当直接提供
<code>inputs_embeds</code>时，模型无法区分哪些位置是填充的（因嵌入已生成，原始输入信息丢失）。此时，函数
<strong>默认所有位置有效</strong> ，生成连续的位置ID，并通过避开
<code>padding_idx</code>确保即使存在填充，位置嵌入也不会错误引用填充索引。</li>
<li><strong>保持一致性</strong> ： 在标准流程中，模型会根据
<code>input_ids</code>处理填充位置（如将填充token的位置ID设为0）。但当无法获取
<code>input_ids</code>时，此函数采用保守策略，生成不与
<code>padding_idx</code>冲突的ID，以模拟非填充场景，避免引入意外偏差。</li>
</ol>
<p>在Roberta模型中，创建位置IDs时考虑
<code>padding_idx</code>的主要原因是为了避免生成的位置ID与嵌入层中的填充索引冲突，确保位置嵌入的有效性。具体原因如下：</p>
<ol type="1">
<li><strong>避免位置嵌入的无效化</strong> ：
位置嵌入层（<code>Embedding</code>）中的
<code>padding_idx</code>对应的向量通常被设置为零且不参与训练。若位置ID等于
<code>padding_idx</code>，该位置的位置嵌入会被固定为零，失去学习能力。通过从
<code>padding_idx + 1</code>开始生成位置ID（如
<code>padding_idx=0</code>时，ID从1开始），所有有效位置均获得非填充索引，确保位置嵌入被正常学习和使用。</li>
<li><strong>处理输入的不确定性</strong> ： 当直接提供
<code>inputs_embeds</code>时，模型无法区分哪些位置是填充的（因嵌入已生成，原始输入信息丢失）。此时，函数
<strong>默认所有位置有效</strong> ，生成连续的位置ID，并通过避开
<code>padding_idx</code>确保即使存在填充，位置嵌入也不会错误引用填充索引。</li>
<li><strong>保持一致性</strong> ： 在标准流程中，模型会根据
<code>input_ids</code>处理填充位置（如将填充token的位置ID设为0）。但当无法获取
<code>input_ids</code>时，此函数采用保守策略，生成不与
<code>padding_idx</code>冲突的ID，以模拟非填充场景，避免引入意外偏差。</li>
</ol>
</blockquote>
<p>所以确实和input_ids时的处理稍有不同，下面是id对应的函数请看：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_position_ids_from_input_ids</span>(<span class="hljs-params">input_ids, padding_idx, past_key_values_length=<span class="hljs-number">0</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols</span><br><span class="hljs-string">    are ignored. This is modified from fairseq&#x27;s `utils.make_positions`.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        x: torch.Tensor x:</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns: torch.Tensor</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.</span><br>    mask = input_ids.ne(padding_idx).<span class="hljs-built_in">int</span>()<br>    incremental_indices = (torch.cumsum(mask, dim=<span class="hljs-number">1</span>).type_as(mask) + past_key_values_length) * mask<br>    <span class="hljs-keyword">return</span> incremental_indices.long() + padding_idx<br></code></pre></td></tr></table></figure>
<p>那么当padding_idx设置为3的时候，位置id就从4开始。那前面的0，1，2怎么办呢？ds说为了避免和填充索引冲突，这三个就不会被用作位置id了。</p>
<p>*不知道其他模型的实现里是否有这些问题，但请注意</p>
<h2 id="参考参考">参考参考</h2>
<p><a
target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/v4.48.2/src/transformers/models/roberta/modeling_roberta.py#L1669">roberta源代码</a></p>
<p><a
target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/issues/10736#issuecomment-800175342">roberta
pad tokens</a></p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/note/" class="category-chain-item">note</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>roberta模型你在干嘛</div>
      <div>https://patchaven.github.io/2025/02/08/robertawhy/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>ph-bear</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年2月8日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/01/15/vcf2fasta/" title="vcf2fasta">
                        <span class="hidden-mobile">vcf2fasta</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>





  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
